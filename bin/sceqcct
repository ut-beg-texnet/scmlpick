#!/usr/bin/env python3

"""
Copyright (C) by TexNet/CISR
Created on Jul 31 2024
Author of the Software: Camilo Munoz
"""

import logging as stdlogging
import sys, os, ray, time, threading, traceback
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from seiscomp.client import StreamApplication, Inventory, Protocol
from seiscomp.datamodel import Pick, TimeQuantity, WaveformStreamID, Phase, NotifierMessage, Notifier, OP_ADD, OP_UPDATE, PickReference, Comment, CreationInfo, Origin
from seiscomp.core import Time, FloatArray
from seiscomp import logging, geo
from obspy import Stream, Trace, UTCDateTime
from pathlib import Path
from seiscomp.system import Environment
from multiprocessing import Pool
share_path = os.path.join(Environment.Instance().shareDir(), "sceqcct/tools")
sys.path.append(share_path)
from predictor import mseed_predictor
import fnmatch
import psutil

# import main_picks as picks2xml

class sceqcct(StreamApplication):
    def __init__(self):
        StreamApplication.__init__(self, len(sys.argv), sys.argv)
        self.setDatabaseEnabled(True, True)
        self.setPrimaryMessagingGroup(Protocol.LISTENER_GROUP)
        self.setMessagingEnabled(True)
        # self.setPrimaryMessagingGroup("PICKEQCCT")
        # self.addMessagingSubscription("PICK")
        self.setLoadInventoryEnabled(True)

        self.ei = Environment.Instance()
        # share_path = os.path.join(self.ei.shareDir(), "sceqcct/tools")
        # # share_path = os.path.abspath("../share/sceqcct/tools")
        # sys.path.append(share_path)
        # self.predictor = __import__('predictor')
        self.initStreams = []
        self.streams = []
        self.stream = Stream()
        self.streamPick = Stream()
        self.delayStream = Stream()
        self.oneMinuteStream = Stream()
        self.oneMinuteStDelay = Stream()
        self.stasDict = {}
        self.stasPick = set()
        self.delayStasPick = set()
        self.staSend = []
        self.delayStaSend = []
        self.wait = False
        # Shared list of task IDs
        self.tasks_ids = []
        self.profiles = []
        # Control variable to stop monitoring when there are no more tasks
        self.tasks_ids_end = threading.Event()

    def initConfiguration(self):
        """
        Read configuration file.
        """
        if not StreamApplication.initConfiguration(self):
            return False
        try:
            self.stations = self.configGetStrings("stations")
        except Exception as e:
            logging.info('List of stations could not be read: %s' % e)
        try:
            self.whiteChanns = self.configGetStrings("whiteChanns")
        except Exception as e:
            logging.info('whiteChanns could not be read: %s' % e)
        try:
            self.eqcct_Pmodel = self.configGetString('eqcct.Pmodel')
        except Exception as e:
            logging.info('P model could not be read : %s' % e)
        try:
            self.eqcct_Smodel = self.configGetString('eqcct.Smodel')
        except Exception as e:
            logging.info('S model could not be read : %s' % e)   
        try:
            self.windowLength = self.configGetInt('eqcct.windowLength')
        except Exception as e:
            logging.info('windowLength could not be read : %s' % e)
        try:
            self.filterShift = self.configGetDouble('eqcct.filterShift')
        except Exception as e:
            logging.info('filterShift could not be read : %s' % e)  
        try:
            self.probThreshold = self.configGetDouble('eqcct.probThreshold')
        except Exception as e:
            logging.info('probThreshold could not be read : %s' % e)
        try:
            self.timeShift = self.configGetInt('eqcct.timeShift')
        except Exception as e:
            logging.info('timeShift could not be read : %s' % e)
        try:
            self.minStasBulk = self.configGetInt('eqcct.minStasBulk')
        except Exception as e:
            logging.info('minStasBulk could not be read : %s' % e)
        try:
            self.minDelayStasBulk = self.configGetInt('eqcct.minDelayStasBulk')
        except Exception as e:
            logging.info('minDelayStasBulk could not be read : %s' % e)
        try:
            self.firstWait = self.configGetInt('eqcct.firstWait')
        except Exception as e:
            logging.info('firstWait could not be read : %s' % e)
        try:
            self.secondWait = self.configGetInt('eqcct.secondWait')
        except Exception as e:
            logging.info('secondWait could not be read : %s' % e)
        try:
            self.maxWait = self.configGetInt('eqcct.maxWait')
        except Exception as e:
            logging.info('maxWait could not be read : %s' % e)
        try:
            self.startLatency = self.configGetInt('eqcct.startLatency')
        except Exception as e:
            logging.info('startLatency could not be read : %s' % e)
        try:
            self.timeRemoveTrace = self.configGetInt('eqcct.timeRemoveTrace')
        except Exception as e:
            logging.info('timeRemoveTrace could not be read : %s' % e)
        try:
            self.traceDelay = self.configGetInt('eqcct.traceDelay')
        except Exception as e:
            logging.info('traceDelay could not be read : %s' % e)



        #EQCCT
        try:
            self.eqcctPthr = self.configGetDouble('eqcct.eqcctPthr')
        except Exception as e:
            logging.info('eqcctPthr could not be read : %s' % e)
        try:
            self.eqcctSthr = self.configGetDouble('eqcct.eqcctSthr')
        except Exception as e:
            logging.info('eqcctSthr could not be read : %s' % e)
        try:
            self.eqcctOverlap = self.configGetInt('eqcct.eqcctOverlap')
        except Exception as e:
            logging.info('eqcctOverlap could not be read : %s' % e)
        try:
            self.eqcctBatchSize = self.configGetInt('eqcct.eqcctBatchSize')
        except Exception as e:
            logging.info('eqcctBatchSize could not be read : %s' % e)
        try:
            self.gpuID = self.configGetInt('eqcct.gpuID')
        except Exception as e:
            logging.info('gpuID could not be read : %s' % e)
        try:
            self.gpuLimit = self.configGetInt('eqcct.gpuLimit')
        except Exception as e:
            logging.info('gpuLimit could not be read : %s' % e)

        self.profilesConfig()

        #RAY
        try:
            self.numCPUs = self.configGetInt('ray.numCPUs')
        except Exception as e:
            logging.info('numCPUs could not be read : %s' % e)

        try:
            self.maxTimeTasksQueue = self.configGetInt('ray.maxTimeTasksQueue')
        except Exception as e:
            logging.info('maxTimeTasksQueue could not be read : %s' % e)

        try:
            self.maxStsTasksQueue = self.configGetInt('ray.maxStsTasksQueue')
        except Exception as e:
            logging.info('maxStsTasksQueue could not be read : %s' % e)

        #qcheck
        try:
            self.latUncTHR = self.configGetInt('qcheck.latUncTHR')
        except Exception as e:
            logging.info('latUncTHR could not be read : %s' % e)

        try:
            self.lonUncTHR = self.configGetInt('qcheck.lonUncTHR')
        except Exception as e:
            logging.info('lonUncTHR could not be read : %s' % e)

        try:
            self.depthUncTHR = self.configGetInt('qcheck.depthUncTHR')
        except Exception as e:
            logging.info('depthUncTHR could not be read : %s' % e)

        try:
            self.depthTHR = self.configGetInt('qcheck.depthTHR')
        except Exception as e:
            logging.info('depthTHR could not be read : %s' % e)

        try:
            self.azGapTHR = self.configGetInt('qcheck.azGapTHR')
        except Exception as e:
            logging.info('azGapTHR could not be read : %s' % e)

        # try:
        #     self.region = self.configGetString('qcheck.region')
        # except Exception as e:
        #     logging.info('region could not be read : %s' % e)

        try:
            self.TrueOrgEvalStat = self.configGetInt('qcheck.TrueOrgEvalStat')
        except Exception as e:
            logging.info('TrueOrgEvalStat could not be read : %s' % e)

        try:
            self.FalseOrgEvalStat = self.configGetInt('qcheck.FalseOrgEvalStat')
        except Exception as e:
            logging.info('FalseOrgEvalStat could not be read : %s' % e)

        return True

    def init(self):
        if not StreamApplication.init(self): return False
        # self.enableTimer(self.latencyPeriod)
        self.updateStreams()
        self.now = UTCDateTime(Time.GMT())
        self.startTime = UTCDateTime(pd.to_datetime(str(self.now)) + timedelta(seconds=self.startLatency))
        logging.info("Starting time %s " % self.now)
        logging.info("Starting time plus start latency %s " % self.startTime)
        predictorPath = os.path.join(self.ei.shareDir(), "sceqcct/tools")
        # Creating pool for multiprocessing 
        # pool_size = 4 # New cfg file parameter
        # self.pool = Pool(processes=pool_size)

        # Start a thread to monitor the results without blocking the main process
        self.monitorThread = threading.Thread(target=self.monitorTasks, args=(self.tasks_ids,))
        self.monitorThread.start()

        # Allocates X amount of CPUs for the function of creating tasks 
        ray.init(ignore_reinit_error=True, num_cpus=self.numCPUs, logging_level=stdlogging.FATAL, log_to_driver=False, runtime_env={"working_dir": predictorPath})
        logging.info("Allocates %s amount of CPUs for the function of creating tasks with ray" % self.numCPUs)
        return True

    def profilesConfig(self):
        profiles = []
		# try:
        profiles = self.configGetStrings('pipelines.profiles')
		# except:
			# logging.error('Error loading profiles' )
			# sys.exit(-1)
        if len(profiles) != 0:
            for profile in profiles:
                profDic = {}
                profDic['name'] = profile
                try:
                    profDic['pickTargetGroup'] = self.configGetString('pipelines.profiles.'+profile+'.pickTargetGroup')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.pickTargetGroup')
                    logging.warning('setting default pickTargetGroup')
                    profDic['pickTargetGroup'] = ''
                    pass

                try:
                    profDic['locTargetGroup'] = self.configGetString('pipelines.profiles.'+profile+'.locTargetGroup')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.locTargetGroup')
                    logging.warning('setting default locTargetGroup')
                    profDic['locTargetGroup'] = ''
                    pass

                try:
                    profDic['authorTarget'] = self.configGetString('pipelines.profiles.'+profile+'.authorTarget')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.authorTarget')
                    logging.warning('setting default authorTarget')
                    profDic['authorTarget'] = ''
                    pass

                try:
                    profDic['stations'] = self.configGetStrings('pipelines.profiles.'+profile+'.stations')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.stations')
                    logging.warning('setting stations = ""')
                    profDic['stations'] = ''
                    pass

                try:
                    profDic['region'] = self.configGetString('pipelines.profiles.'+profile+'.region')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.region')
                    logging.warning('setting stations = ""')
                    profDic['region'] = ''
                    pass

                try:
                    profDic['filtersFile'] = self.configGetString('pipelines.profiles.'+profile+'.filtersFile')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.filtersFile')
                    logging.warning('setting filtersFile = ""')
                    profDic['filtersFile'] = ''
                    pass
                self.profiles.append(profDic)
        else:
            logging.warning('No profiles configured. Nothing to do')
            pass

    def updateStreams(self):
        # streamIDs = self.configStreams()
        streamIDs = self.currentStreams()
        for streamID in streamIDs:
            for sta in self.stations:
                staCheck = False
                if '*' in sta:
                    if (streamID[0] == sta.split('.')[0] and fnmatch.fnmatch(streamID[1], sta.split('.')[1])):
                        staCheck = True
                else:
                    if (streamID[0] == sta.split('.')[0] and streamID[1] == sta.split('.')[1]):
                        staCheck = True
                if staCheck:
                        if streamID[1] in self.stasDict:
                            self.stasDict[streamID[1]].append(streamID[3])
                        else:
                            self.stasDict[streamID[1]] = [streamID[3]]
                        self.initStreams.append(streamID)
                        self.recordStream().addStream(*streamID)
        return

    def configStreams(self):
        streamIDs=[]
        cfg = self.configModule()
        inv = Inventory.Instance().inventory()
        for i in range(cfg.configStationCount()):
            cfg_sta = cfg.configStation(i)
            net = cfg_sta.networkCode()
            sta = cfg_sta.stationCode()
            nets = inv.networkCount()
            for n in range(nets):
                network = inv.network(n)
                if network.code() == net:
                    stas = network.stationCount()
                    for s in range(stas):
                        station = network.station(s)
                        if station.code() == sta:
                            locs = station.sensorLocationCount()
                            for l in range(locs):
                                location = station.sensorLocation(l)
                                loc = location.code()
                                chans = location.streamCount()
                                for c in range(chans):
                                    channel = location.stream(c)
                                    chan = channel.code()
                                    if chan[0:2] in self.whiteChanns:
                                        streamIDs.append([net, sta, loc, chan])
        return streamIDs

    def currentStreams(self):
        now = Time.GMT()
        streamIDs=[]
        inv = Inventory.Instance()
        nets = inv.inventory().networkCount()
        for n in range(nets):
            network = inv.inventory().network(n)
            net = network.code()
            stas = network.stationCount()
            for s in range(stas):
                station = network.station(s)
                sta = station.code()
                station_now = False
                station_now = inv.getStation(network.code(),sta,now)
                if station_now:
                    staCode = station_now.code()
                    locs = station_now.sensorLocationCount()
                    for l in range(locs):
                        location = station_now.sensorLocation(l)
                        loc = location.code()
                        loc_now = False
                        loc_now = inv.getSensorLocation(network.code(),staCode,loc,now)
                        if loc_now:
                            locCode = loc_now.code()
                            chans = location.streamCount()
                            for c in range(chans):
                                channel = location.stream(c)
                                chan = channel.code()
                                if chan[0:2] in self.whiteChanns:
                                    chan_now = False
                                    chan_now = inv.getStream(network.code(),staCode,locCode,chan,now)
                                    if chan_now:
                                        chanCode = chan_now.code()
                                        sid = [net, staCode, locCode, chanCode]
                                        if sid not in streamIDs:
                                            streamIDs.append([net, staCode, locCode, chanCode])
        return streamIDs
                
    def handleRecord(self, rec):

        if rec.data():
            dataObj = FloatArray.Cast(rec.data())
            if dataObj:
                data = [dataObj.get(i) for i in range(dataObj.size())]
            else:
                logging.debug('No data')
                return
        else:
            logging.debug('No data')
            return

        startTime = rec.startTime()
        endTime =  rec.endTime()
        net = rec.networkCode()
        sta = rec.stationCode()
        net_sta = f"{net}.{sta}"
        loc = rec.locationCode()
        chan = rec.channelCode()
        sampFreq = rec.samplingFrequency()
        start_time = pd.to_datetime(str(startTime))
        end_time = pd.to_datetime(str(endTime))
        traceID = f'{net}.{sta}.{loc}.{chan}'

        try:
            data = np.array(data)
        except:
            logging.debug('No data')
            return

        header = {
            'network': net, 
            'station': sta, 
            'location': loc, 
            'channel': chan, 
            'npts': len(data), 
            'sampling_rate': sampFreq,
            'starttime': UTCDateTime(start_time)
        }
        tr = Trace(data=data, header=header)
        if tr is None or len(tr.data) == 0:
            return
        
        if self.stream:
            index, trace = next(((i, t) for i, t in enumerate(self.stream) if t.stats.network == net and t.stats.station == sta and t.stats.channel == chan),(None, None))
            if index is not None and trace is not None:
                tempSt = Stream([trace, tr])
                tempSt.merge(method=1, fill_value=0)
                tnew =  tempSt[0]
                self.stream[index] = tnew
                #self.stream[self.stream.index(trace)] = tnew
            else:
                self.stream.append(tr)
                return
            if not trace or (self.stream[index].stats.endtime - self.stream[index].stats.starttime) <= (self.windowLength + self.filterShift) or self.stream[index].stats.endtime <= self.startTime:
                return
        else:
            self.stream.append(tr)
            return

        now = UTCDateTime(Time.GMT())
        self.recordStream().setEndTime(Time.GMT())

        logging.info(f"TIME NOW")
        logging.info(f'{now}')

        if now < self.startTime:
            logging.debug('Waiting for the processing start time %s' % self.startTime)

        logging.info(f"START TIME")
        logging.info(f'{startTime}')

        logging.info(f"END TIME")
        logging.info(f'{endTime}')

        logging.info(f"STATION")
        logging.info(f'{rec.stationCode()}')

        timeMark1 = self.startTime - self.filterShift
        timeMark2 = self.startTime + self.timeShift - self.filterShift
        timeMarks = [timeMark1,timeMark2]

        if self.stream[index].stats.endtime < self.startTime +  timedelta(seconds=self.windowLength):
            self.stream[index].trim(self.startTime-self.filterShift, UTCDateTime(end_time), pad=True, fill_value=0)
            return
        else:
            in_window1 = abs(self.stream[index].stats.starttime.second - timeMarks[0].second) < 0.5
            in_window2 = abs(self.stream[index].stats.starttime.second - timeMarks[1].second) < 0.5
            if not in_window1 and not in_window2:
                logging.info(f"Start time not in timeMarks: {self.stream[index].stats.starttime} not in {timeMarks}")
                logging.info(f"Trace {self.stream[index]}")
                newTime = self.stream[index].stats.starttime.replace(second=timeMarks[0].second) if timeMarks[0].second < self.stream[index].stats.starttime.second else (self.stream[index].stats.starttime - 60).replace(second=timeMarks[1].second)
                try:
                    self.stream[index].trim(newTime, UTCDateTime(end_time), pad=True, fill_value=0)
                    logging.info(f"Trimmed using closest allowed timestamp {newTime}")
                except Exception as e:
                    logging.error("Error trimming trace {} with error {}".format(self.stream[index],e))

        if self.stream[index].stats.endtime - self.stream[index].stats.starttime >= self.windowLength + self.filterShift:
            index_found = next((j for j, tr in enumerate(self.streamPick) if tr.id == traceID), None)
            if index_found is None:
                trDelay = UTCDateTime(Time.GMT()) - self.stream[index].stats.starttime
                if trDelay > self.traceDelay:
                    index2_found = next((k for k, tr2 in enumerate(self.delayStream) if tr2.id == traceID), None)
                    if index2_found is None:
                        self.delayStream.append(self.stream[index])
                        self.delayStasPick.add(self.stream[index].stats.station)
                    else:
                        self.delayStream[index2_found] = self.stream[index]
                else:
                    self.streamPick.append(self.stream[index])
                    self.stasPick.add(self.stream[index].stats.station)   
            else:
                    self.streamPick[index_found] = self.stream[index]
        else:
            return
       
        self.checkCommonChan(self.stream[index])

        #if self.streamPick:
        #    logging.info("STREAM PICK")
        #    logging.info("\n".join(str(trace) for trace in self.streamPick))

        #if self.delayStream:
        #    logging.info("DELAY STREAM PICK")
        #    logging.info("\n".join(str(trace) for trace in self.delayStream))

        # Adding delayed stations with data for all channels avail. to delayStaSend
        self.checkChannDelay(self.stream[index])

        # Adding stations with data for all channels avail. to staSend
        self.checkChann(self.stream[index])

        removed_streams = [trace for trace in self.stream if (trace.stats.endtime - trace.stats.starttime) > self.timeRemoveTrace]
        self.stream = [trace for trace in self.stream if (trace.stats.endtime - trace.stats.starttime) <= self.timeRemoveTrace]

        if removed_streams:
            logging.info(f"Removed streams, (length > {self.timeRemoveTrace} secs):")
            logging.info("\n".join(str(trace) for trace in removed_streams))

        # Check delay for stations in memory
        staSet = set()
        for i,trace in enumerate(self.streamPick):
            sta = self.streamPick[i].stats.station
            if sta in self.staSend and sta not in staSet:
                delayEnd = UTCDateTime(Time.GMT()) - trace.stats.endtime # Leave here but this time is different than the one use in oneMinuteStream (to evaluate).
                delayStart = UTCDateTime(Time.GMT()) - trace.stats.starttime # Same than oneMinuteStream this is the correct time to count the delay.
                if delayEnd >= self.firstWait or delayStart >= self.firstWait + self.windowLength:
                    staSet.add(sta)
        
        sendNow = False
        if self.wait:
            delay = UTCDateTime(Time.GMT()) - self.sendTime
            if delay >= self.maxWait:
                self.wait = False
            elif delay >= self.secondWait and len(self.staSend) > 0:
                sendNow = True

        params = {
            "filterShift" : self.filterShift,
            "share_path" : self.ei.shareDir(),
            "P_threshold": self.eqcctPthr,
            "S_threshold": self.eqcctSthr,
            "normalization_mode": "std",
            "overlap": self.eqcctOverlap,
            "batch_size": self.eqcctBatchSize,
            "overwrite": True,
            # "model": model,
            "p_model": self.eqcct_Pmodel,
            "s_model": self.eqcct_Smodel,
            "numCPUs": self.numCPUs,
            "gpu_id": self.gpuID,
            "gpu_limit": self.gpuLimit,
            "maxStsTasksQueue": self.maxStsTasksQueue,
            }

        for profile in self.profiles:
            if net_sta in profile["stations"]:
                try:
                    params["df_filters"] = pd.read_csv(profile["filtersFile"], header=None, names=["sta", "hp", "lp"])
                except:
                    params["df_filters"] = None

        if (len(self.staSend) >= self.minStasBulk and len(staSet) >= self.minStasBulk and self.wait == False) or sendNow:
            if sendNow:
                logging.info(f"Sending now due to second delay")
                self.wait = False
            else:
                logging.info(f"Sending now due to first delay")
                self.sendTime = UTCDateTime(Time.GMT())
                self.wait = True
            # self.minuteStream('realTime',params)
            self.minuteStream(params)
        
        if len(self.delayStaSend) >= self.minDelayStasBulk:
            logging.info(f"Sending now delayed stations")
            # self.minuteStream('delayed',params)
            self.minuteStreamDelay(params)

    def checkCommonChan(self, tnew):
        if not tnew:
            return
        station_tnew = tnew.stats.station
        if any(trace.stats.station == station_tnew for trace in self.streamPick):
            traces_to_move = [trace for trace in self.delayStream if trace.stats.station == station_tnew]
            if traces_to_move:
                self.streamPick += Stream(traces_to_move)
                self.delayStream = Stream([trace for trace in self.delayStream if trace.stats.station != station_tnew])

    def minuteStreamDelay(self,params):
        toRemove = []
        # Process the one-minute stream
        for i,trace in enumerate(self.delayStream):
            if self.delayStream[i].stats.station in self.delayStaSend:
                duration = self.delayStream[i].stats.endtime - self.delayStream[i].stats.starttime
                Nminutes = int(duration // self.windowLength)
                if Nminutes > 0:
                    startMinute = self.delayStream[i].stats.starttime
                    endMinute = self.delayStream[i].stats.starttime + (Nminutes*self.windowLength) + self.filterShift
                    traceMin = trace.slice(
                                        starttime=startMinute,
                                        endtime=endMinute
                                        )
                    self.oneMinuteStDelay.append(traceMin)
                    toRemove.append(i)
                else:
                    logging.info(f"This should never happen, Trying to send a trace with less than a minute duration{self.delayStream[i]}")
                    continue

        for i in sorted(toRemove, reverse=True): del self.delayStream[i]

        for i,trace in enumerate(self.stream):
            for tr in self.oneMinuteStDelay:
                if trace.stats.network == tr.stats.network and trace.stats.station == tr.stats.station and trace.stats.channel == tr.stats.channel:
                    try:
                        self.stream[i] = trace.slice(
                            starttime=tr.stats.endtime - self.timeShift - self.filterShift,
                            endtime=self.stream[i].stats.endtime,
                            )
                    except:
                        logging.error(f"Error when slice the trace %s" % self.stream[i])
            
        logging.info(f"ONE MINUTE STREAM")
        for i, trace in enumerate(self.oneMinuteStDelay):
            logging.info(f'{trace}')
            
        # Run EQCCT picker
        # self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStDelay,params))
        self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStDelay,params))
        logging.info(f"Task for data {startMinute}, {endMinute}, {self.oneMinuteStDelay} assigned.")
        logging.info(f"The main process continues to generate and launch tasks.")

        # resultados = ray.get(self.tasks_ids)
        # print("Resultados de las tareas secundarias:", resultados)

        # self.sendTask(startMinute,endMinute,self.oneMinuteStDelay)
        # self.run_picker()

        self.oneMinuteStDelay = Stream()
        self.delayStaSend = []

    def minuteStream(self,params):
        toRemove = []
        # Process the one-minute stream
        for i,trace in enumerate(self.streamPick):
            if self.streamPick[i].stats.station in self.staSend:
                duration = self.streamPick[i].stats.endtime - self.streamPick[i].stats.starttime
                Nminutes = int(duration // self.windowLength)
                if Nminutes > 0:
                    startMinute = self.streamPick[i].stats.starttime
                    endMinute = self.streamPick[i].stats.starttime + (Nminutes*self.windowLength) + self.filterShift
                    traceMin = trace.slice(
                                        starttime=startMinute,
                                        endtime=endMinute
                                        )
                    self.oneMinuteStream.append(traceMin)
                    toRemove.append(i)
                else:
                    logging.info(f"This should never happen, Trying to send a trace with less than a minute duration{self.streamPick[i]}")
                    continue

        for i in sorted(toRemove, reverse=True): del self.streamPick[i]

        for i,trace in enumerate(self.stream):
            for tr in self.oneMinuteStream:
                if trace.stats.network == tr.stats.network and trace.stats.station == tr.stats.station and trace.stats.channel == tr.stats.channel:
                    try:
                        self.stream[i] = trace.slice(
                            starttime=tr.stats.endtime - self.timeShift - self.filterShift,
                            endtime=self.stream[i].stats.endtime,
                            )
                    except:
                        logging.error(f"Error when slice the trace %s" % self.stream[i])
            
        logging.info(f"ONE MINUTE STREAM")
        for i, trace in enumerate(self.oneMinuteStream):
            logging.info(f'{trace}')
            
        # Run EQCCT picker
        # self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStream,params))
        self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStream,params))
        logging.info(f"Task for data {startMinute}, {endMinute}, {self.oneMinuteStream} assigned.")
        logging.info(f"The main process continues to generate and launch tasks.")

        # resultados = ray.get(self.tasks_ids)
        # print("Resultados de las tareas secundarias:", resultados)

        # self.sendTask(startMinute,endMinute,self.oneMinuteStream)
        # self.run_picker()

        self.oneMinuteStream = Stream()
        self.staSend = []

    def checkChann(self, tnew):
        if not tnew:
            return  
        station_tnew = tnew.stats.station
        if station_tnew in self.stasPick:
            checkSta = all(
                any(trace.stats.station == station_tnew and recChann == trace.stats.channel for trace in self.streamPick)
                for recChann in self.stasDict.get(station_tnew, [])
            )
            if checkSta and station_tnew not in self.staSend:
                self.staSend.append(station_tnew)
                self.stasPick.discard(station_tnew) 

    def checkChannDelay(self, tnew):
        if not tnew:
            return  
        station_tnew = tnew.stats.station
        if station_tnew in self.delayStasPick:
            checkSta = all(
                any(trace.stats.station == station_tnew and recChann == trace.stats.channel for trace in self.delayStream)
                for recChann in self.stasDict.get(station_tnew, [])
            )
            if checkSta and station_tnew not in self.delayStaSend:
                self.delayStaSend.append(station_tnew)
                self.delayStasPick.discard(station_tnew) 

    @staticmethod
    @ray.remote
    def run_picker(start_time,end_time,stream,params):
        
        # Create working dirs 
        #Path(os.path.join(self.ei.shareDir(), "sceqcct/results/picks")).mkdir(exist_ok=True, parents=True)
        #Path(os.path.join(self.ei.shareDir(), "sceqcct/results/logs/picker")).mkdir(exist_ok=True, parents=True)
        #pathResutls = os.path.join(self.ei.shareDir(), "sceqcct/results")

        # Load EQCCT model                                        
        # model = self.predictor.load_model(self.eqcct_Pmodel, self.eqcct_Smodel, f"{pathResutls}/model.log")

        # Get picker tasks
        start_time = UTCDateTime(start_time)
        end_time = UTCDateTime(end_time)
        times_list = [[start_time, end_time]]

        #log_file_run = f"{pathResutls}/logs/picker/eqcct_run.log"
        #logging.info(f"[{datetime.now()}] Running EQCCT picker. Log file: {log_file_run}")

        # Prepare tasks for EQCCT picking
        stations = []
        for trace in stream:
            # station = f'{trace.stats.network}.{trace.stats.station}.{trace.stats.location}.{trace.stats.channel}'
            station = f'{trace.stats.network}.{trace.stats.station}'
            if station not in stations:
                stations.append(station)


        tasks = [[f"({i+1}/{len(times_list)})", times_list[i][0].strftime(format="%Y%m%dT%H%M%SZ"), times_list[i][1].strftime(format="%Y%m%dT%H%M%SZ"), stations] for i in range(len(times_list))]

        # Submit tasks to ray in a queue
        # tasks_queue = []
        # logging.debug(f"[{datetime.now()}] Started EQCCT picking process.\n")
        # for i in range(len(tasks)):
        #     while True:
        #         # Add new task to queue while max is not reached
        #         if len(tasks_queue) < self.maxTimeTasksQueue:
        #             tasks_queue.append(self.picker.remote(tasks[i],stream))
        #             break
        #         # If there are more tasks than maximum, just process them
        #         else:
        #             tasks_finished, tasks_queue = ray.wait(tasks_queue, num_returns=1, timeout=None)
        #             for finished_task in tasks_finished:
        #                 log_entry = ray.get(finished_task)
        #                 logging.info(log_entry + "\n")

        # # After adding all the tasks to queue, process what's left
        # while tasks_queue:
        #     tasks_finished, tasks_queue = ray.wait(tasks_queue, num_returns=1, timeout=None)
        #     for finished_task in tasks_finished:
        #         log_entry = ray.get(finished_task)
        #         logging.info(log_entry + "\n")
        # ray.shutdown()

        logging.debug('Sending stations to EQCCT %s'% stations)
        tasks = [[f"({i+1}/{len(times_list)})", times_list[i][0], times_list[i][1], stations] for i in range(len(times_list))]

        # Run picker
        for task in tasks:
            picks = sceqcct.picker(task,stream,params)

        return picks

    def monitorTasks(self, tasks_ids):
        results = []
        
        while tasks_ids or not self.tasks_ids_end.is_set():
            # We check the completed results without blocking
            try:
                if tasks_ids:
                    results_ready, tasks_ids[:] = ray.wait(tasks_ids, num_returns=len(tasks_ids), timeout=1)
                
                    # We process the results that are ready
                    for result_id in results_ready:
                        try:
                            result = ray.get(result_id)
                            logging.info(f"Result received on the monitor: {result}")
                            self.scPhase(result)
                            results.append(result)
                        except Exception as task_error:
                            logging.error("Error while processing task {}".format(task_error))
                            logging.error(traceback.format_exc())
                time.sleep(1)  # Short pause to avoid excessive CPU usage
                logging.info(f"Pending tasks on the monitor: {len(tasks_ids)}")
            except Exception as e:
                logging.error("Task Monitor Error {}".format(e))
                logging.error(traceback.format_exc())

        logging.info(f"The monitor has processed all the results.")
        return results

        # Call the function asynchronously
        # print('SEND TASK')
        # result = self.pool.apply_async(sceqcct.run_picker, (startMinute, endMinute, oneMinuteStream, params))
        # print(result.get())

        # task = sceqcct.run_picker.remote(startMinute,endMinute,oneMinuteStream,params)
        # self.tasks.append(task)
        # logging.info(f"Task for data {startMinute}, {endMinute}, {oneMinuteStream} assigned.")

        # # Process results as they are ready without blocking
        # async def process_results():
        #     while self.tasks:
        #         # Wait for any task to be ready
        #         ready, self.tasks = ray.wait(self.tasks, num_returns=1, timeout=1)

        #         # Process the result of the ready task
        #         for task in ready:
        #             result = ray.get(task)  # Retrieve the result
        #             print(f"Processed result: {result}")
        #             self.scPhase(result)
        #         # Main process can continue while tasks are still pending
        #         print("Main process continues working...")
        # # Continue doing other work in the main process
        # print("Main process doing other work while Ray tasks run in background...")

        # # Run the result processing concurrently with other tasks
        # asyncio.create_task(process_results())

    @staticmethod
    def picker(task,stream,params):

        picks = None
        eqcctPath = os.path.join(params['share_path'], "sceqcct/tools")
        # share_path = os.path.abspath("../share/sceqcct/tools")
        sys.path.append(eqcctPath)
        predictor = __import__('predictor')

        pos, starttime, endtime, stations = task
        # begin = starttime.strftime(format="%Y%m%dT%H%M%SZ")
        # end = endtime.strftime(format="%Y%m%dT%H%M%SZ")

        #log_file = os.path.join(self.ei.shareDir(), f"sceqcct/results/logs/picker/{begin}_{end}.log")
        #logging.info(f"[{datetime.now()}] Running EQCCT picker {pos}. Log file: {log_file}")

        #output_eqcct = os.path.join(self.ei.shareDir(), f"sceqcct/results/eqcct/{begin}_{end}")
        
        picks = mseed_predictor(stream = stream,
                        filterShift    = params['filterShift'],
                        # output_dir    = output_eqcct,
                        stations2use  = stations,
                        P_threshold   = params['P_threshold'],
                        S_threshold   = params['S_threshold'],
                        normalization_mode = params['normalization_mode'],
                        overlap       = params['overlap'],
                        batch_size    = params['batch_size'],
                        overwrite     = params['overwrite'],
                        # log_file      = log_file,
                        # model         = model,
                        p_model       = params['p_model'],
                        s_model       = params['s_model'],
                        numCPUs       = params['numCPUs'],
                        gpu_id        = params['gpu_id'],
                        gpu_limit     = params['gpu_limit'],
                        maxStsTasksQueue = params['maxStsTasksQueue'],
                        stations_filters = params["df_filters"])
        
        return picks
        # picks2xml(input_file=output_eqcct, output_file=f"./{pathResutls}/picks/picks_{begin}_{end}.xml", ai='eqcc', thr_dict=self.probThreshold)

    def scPhase(self,picks):
        for pickData in picks:
            logging.info(str(pickData))
            if pickData['PdateTime']:
                PpickID = self.scPick(pickData, 'P', 'PdateTime', 'p_prob')
                if pickData['SdateTime']:
                    pickData['PpickID'] = PpickID
                    self.scPick(pickData, 'S', 'SdateTime', 's_prob')

    def scPick(self,pickData,phaseHint,strDate,strProp):
        pick = Pick.Create()
        Ptime = pickData[strDate]
        scPtime = Ptime.strftime('%Y-%m-%d %H:%M:%S.%f')
        scPtime = Time()
        scPtime.set(Ptime.year,Ptime.month,Ptime.day,Ptime.hour,Ptime.minute,Ptime.second,Ptime.microsecond)
        timeQ = TimeQuantity()
        timeQ.setValue(scPtime)
        # timeQ.setUncertainty(float(pickData[strProp]))
        pick.setTime(timeQ)
        propValue = str(pickData[strProp])
        comm = Comment()
        comm.setId('eqcctProb')
        comm.setText(propValue)
        ci = CreationInfo()
        ci.setCreationTime(Time().GMT())
        comm.setCreationInfo(ci)
        pick.add(comm)
        net_sta = f"{pickData['trace_name'].split('.')[0]}.{pickData['trace_name'].split('.')[1]}"
        staName = pickData['trace_name'].split('.')[1]
        freqmin = 1.0
        freqmax = 45.0
        filterID = f"BW(2,{freqmin},{freqmax})"
        for profile in self.profiles:
            if net_sta in profile["stations"]:
                try:
                    filters = pd.read_csv(profile["filtersFile"], header=None, names=["sta", "hp", "lp"])
                    freqmin = filters[filters.sta == staName].iloc[0]["hp"]
                    freqmax = filters[filters.sta == staName].iloc[0]["lp"]
                    filterID = f"BW(2,{freqmin},{freqmax})"
                except:
                    pass
        # print(pick.time().value())
        # print(pick.time().uncertainty())
        wfID = WaveformStreamID()
        wfID.setNetworkCode(pickData['trace_name'].split('.')[0])
        wfID.setStationCode(pickData['trace_name'].split('.')[1])
        wfID.setLocationCode(pickData['trace_name'].split('.')[2])
        wfID.setChannelCode(pickData['trace_name'].split('.')[3])
        pick.setWaveformID(wfID)
        phase = Phase()
        phase.setCode(phaseHint)
        pick.setPhaseHint(phase)
        pick.setMethodID('EQCCT')
        pick.setEvaluationMode(1)
        pick.setCreationInfo(ci)
        
        filterAndProp = "  ".join([f"prop:  {propValue}", f"filt: {filterID}"])
        pick.setFilterID(filterAndProp)

        logging.debug("Pick produced by EQCCT %s.%s" % (pick.waveformID().networkCode(), pick.waveformID().stationCode()))
        logging.debug('%s' % pick)
            
        self.sendPick(pick)
        self.sendComt(comm, pick)

        # pickRef = PickReference()
        if phaseHint == 'S':
            # pickRef.setPickID(pickData['PpickID'])
            comt = Comment()
            comt.setId('RefPickID')
            comt.setText(str(pickData['PpickID']))
            ci = CreationInfo()
            ci.setCreationTime(Time().GMT())
            comt.setCreationInfo(ci)
            # print(pick.comment(0).id())
            # print(pick.comment(0).text())    
            pick.add(comt)
            self.sendComt(comt, pick)
        else:
            # pickRef.setPickID(pick.publicID())
            pass

        return pick.publicID()

    def sendComt(self, comt, pick):
        op = OP_ADD
        if comt:
            n = Notifier(pick.publicID(), op, comt)
            msg = NotifierMessage()
            msg.attach(n)
            out = self.connection().send(msg)
            logging.debug("Comment sent? %s" % "Yes" if out else "No, failed" )
            # comtObj = Comment.Cast(comt)
            # Notifier.Enable()
            # n = Notifier("EventParameters", op, comtObj)
            # msg = NotifierMessage()
            # msg.attach(n)
            # out = self.connection().send(msg)
            # logging.debug("Comment sent? %s" % "Yes" if out else "No, failed" )

    def sendPick(self, pick):
        op = OP_ADD
        if pick:
            pickObj = Pick.Cast(pick)
            Notifier.Enable()
            # ep = EventParameters()
            # ep.add(pickObj)
            n = Notifier("EventParameters", op, pickObj)
            msg = NotifierMessage()
            msg.attach(n)
            for profile in self.profiles:
                logging.debug("PROFILE %s" % profile["name"])
                logging.debug("wfID %s" % pickObj.waveformID())
                logging.debug("net %s" % pickObj.waveformID().networkCode())
                netSta = f"{pickObj.waveformID().networkCode()}.{pickObj.waveformID().stationCode()}"
                if netSta in profile["stations"]:
                    target = profile["pickTargetGroup"]
                    if target:
                        logging.debug("Trying to send pick %s to %s" % (netSta,target))
                        out = self.connection().send(target,msg)
                        logging.debug("Pick sent? %s" % "Yes" if out else "No, failed" )
                    else:
                        logging.debug("Trying to send pick %s to default target" % (netSta))
                        out = self.connection().send(msg)
                        logging.debug("Pick sent? %s" % "Yes" if out else "No, failed" )

        # if pickRef:
        #     pickRefObj = PickReference.Cast(pickRef)
        #     Notifier.Enable()
        #     n2 = Notifier("EventParameters", op, pickRefObj)
        #     msg2 = NotifierMessage()
        #     msg2.attach(n2)
        #     out2 = self.connection().send(msg2)
        #     logging.debug("PickReference sent? %s" % "Yes" if out2 else "No, failed" )
        # return out

    def changeOrigStatus(self, origin, EvalStat):
        try:
            origin.setEvaluationStatus(EvalStat)
            logging.debug("Changing origin %s evaluation status %s" % (origin.publicID(),EvalStat))
        except Exception as e:
            logging.error("Error changing origin evaluation status {} with error {}".format(origin.publicID(),e))
        return origin

    def qualityCheckOrigin(self, origin):

        latUnc = origin.latitude().uncertainty()
        lonUnc = origin.longitude().uncertainty()
        try:
            depthUnc = origin.depth().uncertainty()
        except ValueError:
            depthUnc = 0
        depth = origin.depth().value()
        azGap = origin.quality().azimuthalGap()

        thresholds = {'latUnc': self.latUncTHR, 'lonUnc': self.lonUncTHR, 'depthUnc': self.depthUncTHR, 'depth': self.depthTHR, 'azGap': self.azGapTHR}
        values = {'latUnc': latUnc, 'lonUnc': lonUnc, 'depthUnc': depthUnc, 'depth': depth, 'azGap': azGap}

        exceeding = {key: value for key, value in values.items() if value > thresholds[key]}
        
        if exceeding:
            logging.debug(f"Values exceeding thresholds: {exceeding}")
            return False
        else:
            logging.debug(f"Origin fulfills thresholds: {exceeding}")
            return True

    def geographicCheckOrigin(self, region, origin):
        if isinstance(region, str):
            if region.split('.')[-1] == 'bna':
                try:
                    within = self.inPolygon(region, origin.latitude().value(), origin.longitude().value())
                    return within
                except FileNotFoundError:
                    logging.error(f'\n\n\t {region} file not found\n\n')
                    return False
            elif len(region.split(',')) == 4:
                quadrant = region.split(',')
                quadrant = tuple(map(float, quadrant))
                return self.inQuadrant(quadrant, origin)
            else:
                logging.error(f'Provide a quadrant or bna file')
                return False
        elif isinstance(region, tuple):
            return self.inQuadrant(quadrant, origin)
        else:
            logging.error(f'Provide a quadrant or bna file')
            return False

    def inPolygon(self, fileName, lat, lon) -> bool:
        self.fs = geo.GeoFeatureSet()
        if not self.fs.readBNAFile(fileName, None):
            logging.error('Impossible to open the bna file %s'%fileName)
            return False
        
        feature = self.fs.features()[0]
        closed = feature.closedPolygon()
        if not closed:
            logging.error('Please fix this: Polygon %s does not not exist or is not closed'%fileName)
        
        coordinates = geo.GeoCoordinate(lat,lon)
        within = feature.contains(coordinates)
        if not within:
            logging.debug('Origin (lat: %s and lon: %s) is not within polygon %s.' % (lat,lon,fileName))

        return(within)

    def inQuadrant(self, quadrant: tuple, origin) -> bool:
        """Check if event location is in a quadrant

        Parameters
        ----------
        quadrant : Tuple
            Quadrant to check in format (lat_min, lat_max, lon_min,  lon_max)

        Returns
        -------
        bool
            True if event is in quadrant, False if not
        """
        assert len(quadrant) == 4, 'Quadrant must be a tuple with 4 elements'
        assert quadrant[0] < quadrant[1], 'The minimum latitude must be less than the maximum latitude'
        assert quadrant[2] < quadrant[3], 'The minimum longitude must be less than the maximum longitude'
        return (quadrant[0] <= origin.latitude().value() <= quadrant[1]
                and quadrant[2] <= origin.longitude().value() <= quadrant[3])

    def handleOrigin(self, org, op=OP_UPDATE):
        """
        Test origins
        """
        logging.debug("Received origin %s" % org.publicID())
        author = org.creationInfo().author()
        profNames = (list(map(lambda p: p['name'], self.profiles)))
        logging.debug("PROFILES %s" % profNames)
        for profile in self.profiles:
            if author == profile["authorTarget"]:
                try:
                    polygon = profile["region"]
                    if polygon != 'none':
                        checkReg = self.geographicCheckOrigin(polygon, org)
                    else:
                        checkReg = True
                    checkQc = self.qualityCheckOrigin(org)
                except:
                    logging.error("Error when performing quality checks for the origin %s" % org.publicID())
                    return
                if checkQc and checkReg:
                    org = self.changeOrigStatus(org,self.TrueOrgEvalStat)
                else:
                    org = self.changeOrigStatus(org,self.FalseOrgEvalStat)
                locTarget = profile["locTargetGroup"]
                if org and locTarget:
                    msg = NotifierMessage()
                    n = Notifier("EventParameters", op, org)
                    msg.attach(n)
                    out = self.connection().send(locTarget,msg)
                    logging.debug("Evaluation status updated? %s" % "Yes" if out else "No, failed" )
            else:
                logging.debug("Author %s not in sceqcct profile %s" %(author,profile["name"]))
            
    def addObject(self, parentID, object):
        """
        Call-back function if a new object is received.
        """
        origin = Origin.Cast(object)
        if origin:
            self.handleOrigin(origin, op=OP_UPDATE)

    def updateObject(self, parentID, object):
        """
        Call-back function if an object is updated.
        """
        origin = Origin.Cast(object)
        if origin:
            self.handleOrigin(origin, op=OP_UPDATE)

    def handleClose(self):
        self.tasks_ids_end.set()
        self.monitorThread.join()
        logging.info(f'The main process has completed all tasks.')
        ray.shutdown()

    def handleAutoShutdown(self):
        self.tasks_ids_end.set()
        self.monitorThread.join()
        logging.info(f'The main process has completed all tasks.')
        ray.shutdown()

    def run(self):
        return StreamApplication.run(self)

def set_cpu_affinity(cores):
    """Set CPU affinity for the current process."""
    p = psutil.Process(os.getpid())
    p.cpu_affinity(cores)

if __name__ == '__main__':
    import sys
    allowed_cores = [0, 1]
    set_cpu_affinity(allowed_cores)
    app = sceqcct()
    sys.exit(app())
