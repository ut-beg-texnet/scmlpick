#!/usr/bin/env python3

"""
Copyright (C) by TexNet/CISR
Created on Jul 31 2024
Author of the Software: Camilo Munoz
"""

import logging as stdlogging
from logging.handlers import RotatingFileHandler
import sys, os, ray, time, traceback
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from seiscomp.client import StreamApplication, Inventory, Protocol
from seiscomp.datamodel import Pick, TimeQuantity, WaveformStreamID, Phase, NotifierMessage, Notifier, OP_ADD, OP_UPDATE, PickReference, Comment, CreationInfo, Origin
from seiscomp.core import Time, FloatArray
from seiscomp import logging, geo
from obspy import Stream, Trace, UTCDateTime
from pathlib import Path
from seiscomp.system import Environment
import multiprocessing as mp
from multiprocessing import Value
#from multiprocessing import Pool
#share_path = os.path.join(Environment.Instance().shareDir(), "sceqcct/tools")
#sys.path.append(share_path)
#from predictor import mseed_predictor
from predictor import mseed_predictor
import fnmatch
import psutil
import threading
import queue

# import main_picks as picks2xml

class sceqcct(StreamApplication):
    def __init__(self):
        StreamApplication.__init__(self, len(sys.argv), sys.argv)
        self.setDatabaseEnabled(True, True)
        self.setPrimaryMessagingGroup(Protocol.LISTENER_GROUP)
        self.setMessagingEnabled(True)
        # self.setPrimaryMessagingGroup("PICKEQCCT")
        # self.addMessagingSubscription("PICK")
        self.setLoadInventoryEnabled(True)

        self.ei = Environment.Instance()
        # share_path = os.path.join(self.ei.shareDir(), "sceqcct/tools")
        # # share_path = os.path.abspath("../share/sceqcct/tools")
        # sys.path.append(share_path)
        # self.predictor = __import__('predictor')
        self.initStreams = []
        self.streams = []
        self.stream = Stream()
        self.streamPick = Stream()
        # self.delayStream = Stream()
        self.stasDict = {}
        self.stasPick = set()
        self.delayStasPick = set()
        self.staSend = []
        # self.delayStaSend = []
        self.wait = False
        self.sendNow = False
        # Shared list of task IDs
        self.tasks_ids = []
        self.profiles = []
        # Control variable to stop monitoring when there are no more tasks
        self.tasks_ids_end = threading.Event()
        self.task_queue = queue.Queue()
        self.last_queue_size = 0
        self.numWorkers = 4
        self.all_station_codes = []
        #self.seen_stations = set()

    def initConfiguration(self):
        """
        Read configuration file.
        """
        if not StreamApplication.initConfiguration(self):
            return False
        try:
            self.stations = self.configGetStrings("stations")
        except Exception as e:
            logging.info('List of stations could not be read: %s' % e)
        try:
            self.whiteChanns = self.configGetStrings("whiteChanns")
        except Exception as e:
            logging.info('whiteChanns could not be read: %s' % e)
        try:
            self.eqcct_Pmodel = self.configGetString('eqcct.Pmodel')
        except Exception as e:
            logging.info('P model could not be read : %s' % e)
        try:
            self.eqcct_Smodel = self.configGetString('eqcct.Smodel')
        except Exception as e:
            logging.info('S model could not be read : %s' % e)   
        try:
            self.windowLength = self.configGetInt('eqcct.windowLength')
        except Exception as e:
            logging.info('windowLength could not be read : %s' % e)
        try:
            self.filterShift = self.configGetDouble('eqcct.filterShift')
        except Exception as e:
            logging.info('filterShift could not be read : %s' % e)  
        try:
            self.probThreshold = self.configGetDouble('eqcct.probThreshold')
        except Exception as e:
            logging.info('probThreshold could not be read : %s' % e)
        try:
            self.timeShift = self.configGetInt('eqcct.timeShift')
        except Exception as e:
            logging.info('timeShift could not be read : %s' % e)
        try:
            self.minStasBulk = self.configGetInt('eqcct.minStasBulk')
        except Exception as e:
            logging.info('minStasBulk could not be read : %s' % e)
        try:
            self.minDelayStasBulk = self.configGetInt('eqcct.minDelayStasBulk')
        except Exception as e:
            logging.info('minDelayStasBulk could not be read : %s' % e)
        try:
            self.firstWait = self.configGetInt('eqcct.firstWait')
        except Exception as e:
            logging.info('firstWait could not be read : %s' % e)
        try:
            self.secondWait = self.configGetInt('eqcct.secondWait')
        except Exception as e:
            logging.info('secondWait could not be read : %s' % e)
        try:
            self.maxWait = self.configGetInt('eqcct.maxWait')
        except Exception as e:
            logging.info('maxWait could not be read : %s' % e)
        try:
            self.startLatency = self.configGetInt('eqcct.startLatency')
        except Exception as e:
            logging.info('startLatency could not be read : %s' % e)
        try:
            self.timeRemoveTrace = self.configGetInt('eqcct.timeRemoveTrace')
        except Exception as e:
            logging.info('timeRemoveTrace could not be read : %s' % e)
        try:
            self.traceDelay = self.configGetInt('eqcct.traceDelay')
        except Exception as e:
            logging.info('traceDelay could not be read : %s' % e)



        #EQCCT
        try:
            self.eqcctPthr = self.configGetDouble('eqcct.eqcctPthr')
        except Exception as e:
            logging.info('eqcctPthr could not be read : %s' % e)
        try:
            self.eqcctSthr = self.configGetDouble('eqcct.eqcctSthr')
        except Exception as e:
            logging.info('eqcctSthr could not be read : %s' % e)
        try:
            self.eqcctOverlap = self.configGetInt('eqcct.eqcctOverlap')
        except Exception as e:
            logging.info('eqcctOverlap could not be read : %s' % e)
        try:
            self.eqcctBatchSize = self.configGetInt('eqcct.eqcctBatchSize')
        except Exception as e:
            logging.info('eqcctBatchSize could not be read : %s' % e)
        try:
            self.gpuID = self.configGetInt('eqcct.gpuID')
        except Exception as e:
            logging.info('gpuID could not be read : %s' % e)
        try:
            self.gpuLimit = self.configGetInt('eqcct.gpuLimit')
        except Exception as e:
            logging.info('gpuLimit could not be read : %s' % e)

        self.profilesConfig()

        #RAY
        try:
            self.numCPUs = self.configGetInt('ray.numCPUs')
        except Exception as e:
            logging.info('numCPUs could not be read : %s' % e)

        try:
            self.maxTimeTasksQueue = self.configGetInt('ray.maxTimeTasksQueue')
        except Exception as e:
            logging.info('maxTimeTasksQueue could not be read : %s' % e)

        try:
            self.maxStsTasksQueue = self.configGetInt('ray.maxStsTasksQueue')
        except Exception as e:
            logging.info('maxStsTasksQueue could not be read : %s' % e)

        #qcheck
        try:
            self.latUncTHR = self.configGetInt('qcheck.latUncTHR')
        except Exception as e:
            logging.info('latUncTHR could not be read : %s' % e)

        try:
            self.lonUncTHR = self.configGetInt('qcheck.lonUncTHR')
        except Exception as e:
            logging.info('lonUncTHR could not be read : %s' % e)

        try:
            self.depthUncTHR = self.configGetInt('qcheck.depthUncTHR')
        except Exception as e:
            logging.info('depthUncTHR could not be read : %s' % e)

        try:
            self.depthTHR = self.configGetInt('qcheck.depthTHR')
        except Exception as e:
            logging.info('depthTHR could not be read : %s' % e)

        try:
            self.azGapTHR = self.configGetInt('qcheck.azGapTHR')
        except Exception as e:
            logging.info('azGapTHR could not be read : %s' % e)

        # try:
        #     self.region = self.configGetString('qcheck.region')
        # except Exception as e:
        #     logging.info('region could not be read : %s' % e)

        try:
            self.TrueOrgEvalStat = self.configGetInt('qcheck.TrueOrgEvalStat')
        except Exception as e:
            logging.info('TrueOrgEvalStat could not be read : %s' % e)

        try:
            self.FalseOrgEvalStat = self.configGetInt('qcheck.FalseOrgEvalStat')
        except Exception as e:
            logging.info('FalseOrgEvalStat could not be read : %s' % e)

        return True

    def init(self):
        if not StreamApplication.init(self): return False
        # self.enableTimer(self.latencyPeriod)
        self.updateStreams()
        self.now = UTCDateTime(Time.GMT())
        self.startTime = UTCDateTime(pd.to_datetime(str(self.now)) + timedelta(seconds=self.startLatency))

        # self.global_staSend_count = Value("i", 0)
        # self.global_delayStaSend_count = Value("i", 0)
        # self.global_staSet_count = Value("i", 0)

        self.global_staSend_counts = mp.Array('i', [0]*self.numWorkers)
        # self.global_delayStaSend_counts = mp.Array('i', [0]*self.numWorkers)
        self.global_staSet_counts = mp.Array('i', [0]*self.numWorkers) 
        self.stream_lock = threading.Lock()

        self.all_station_codes = self.stations
        self.worker_station_groups = self.divide_stations(self.all_station_codes, self.numWorkers)
        self.worker_queues = []
        self.worker_processes = []
        self.init_worker_processes()



        logging.info("Starting time %s " % self.now)
        logging.info("Starting time plus start latency %s " % self.startTime)
        # predictorPath = os.path.join(self.ei.shareDir(), "sceqcct/tools")
        # Creating pool for multiprocessing 
        # pool_size = 4 # New cfg file parameter
        # self.pool = Pool(processes=pool_size)

        # Start a thread to monitor the results without blocking the main process
        #self.monitorThread = threading.Thread(target=self.monitorTasks, args=(self.tasks_ids,))
        self.monitorThread = threading.Thread(target=self.monitorTasks)
        self.monitorThread.start()

        # Allocates X amount of CPUs for the function of creating tasks 
        # ray.init(ignore_reinit_error=True, num_cpus=self.numCPUs, logging_level=stdlogging.FATAL, log_to_driver=False, runtime_env={"working_dir": predictorPath})
        ray.init(ignore_reinit_error=True, num_cpus=self.numCPUs, logging_level=stdlogging.FATAL, log_to_driver=False)
        logging.info("Allocates %s amount of CPUs for the function of creating tasks with ray" % self.numCPUs)


        #FIX, HOW TO CHOOSE FILTER USING STATIONS AFTER gather_worker_results. Now appending all csv files.
        df_list = []

        for profile in self.profiles:
            if profile["filtersFile"]:
                try:
                    df = pd.read_csv(profile["filtersFile"], header=None, names=["sta", "hp", "lp"])
                    df_list.append(df)
                    logging.debug(f"Reading filter file {profile['filtersFile']}")
                except Exception as e:
                    logging.error(f"Error reading {profile['filtersFile']}: {e}")
        self.df_filters = pd.concat(df_list, ignore_index=True) if df_list else None

        self.params = {
            "filterShift" : self.filterShift,
            "share_path" : self.ei.shareDir(),
            "P_threshold": self.eqcctPthr,
            "S_threshold": self.eqcctSthr,
            "normalization_mode": "std",
            "overlap": self.eqcctOverlap,
            "batch_size": self.eqcctBatchSize,
            "overwrite": True,
            # "model": model,
            "p_model": self.eqcct_Pmodel,
            "s_model": self.eqcct_Smodel,
            "numCPUs": self.numCPUs,
            "gpu_id": self.gpuID,
            "gpu_limit": self.gpuLimit,
            "maxStsTasksQueue": self.maxStsTasksQueue,
            "df_filters": self.df_filters,
            }

        return True

    def divide_stations(self, stations, num):
        from collections import defaultdict
        grouped = defaultdict(list)
        for i, sta in enumerate(sorted(stations)):
            grouped[i % num].append(sta)
        return list(grouped.values())

    def init_worker_processes(self):

        self.worker_result_queues = []

        for i, group in enumerate(self.worker_station_groups):
            input_queue = mp.Queue()
            result_queue = mp.Queue()
            # p = mp.Process(target=self.worker_loop, args=(i, group, input_queue, result_queue, self.global_staSend_counts, self.global_delayStaSend_counts, self.global_staSet_counts))
            p = mp.Process(target=self.worker_loop, args=(i, group, input_queue, result_queue, self.global_staSend_counts, self.global_staSet_counts))
            p.start()
            self.worker_queues.append(input_queue)
            self.worker_result_queues.append(result_queue)
            self.worker_processes.append(p)

    def worker_loop(self, worker_id, station_list, input_queue, result_queue, global_staSend_counts, global_staSet_counts):
        """Loop executed by each worker process."""

        log_file = f"/home/seiscomp/.seiscomp/log/sceqcct/worker_{worker_id}.log"

        rotating_handler = RotatingFileHandler(
            log_file, maxBytes=5 * 1024 * 1024, backupCount=3
        )

        rotating_handler.setFormatter(stdlogging.Formatter(
            f"%(asctime)s [%(levelname)s/Worker {worker_id}] %(message)s"
        ))

        stdlogging.basicConfig(
            level=stdlogging.DEBUG,
            handlers=[
                rotating_handler,
                stdlogging.StreamHandler(sys.stdout) 
            ]
        )

        core_id = worker_id  # Asegúrate de que worker_id < número de cores físicos
        try:
            os.sched_setaffinity(0, {core_id})
            # stdlogging.info(f"[Worker {worker_id}] Assigned to CPU core {core_id}")
        except Exception as e:
            stdlogging.warning(f"[Worker {worker_id}] Could not set CPU affinity: {e}")

        stream = Stream()
        streamPick = Stream()
        # delayStream = Stream()
        stasPick = set()
        delayStasPick = set()
        staSend = []
        staSet = set()
        # delayStaSend = []

        running = True
        # buffered_msgs = []

        while True:
            
            try:
                msg = input_queue.get(timeout=0.1)
            except queue.Empty:
                continue
            except Exception as e:
                stdlogging.exception(f"[Worker {worker_id}] Unexpected error while reading input_queue: {e}")
                continue
            # except Exception as e:
            #     result_queue.put(("ERROR", worker_id, f"{type(e).__name__}: {e}"))
            #     continue

            # msg = input_queue.get()

            if msg is None:
                break

            elif msg == "PAUSE":
                running = False
                result_queue.put("PAUSED")
                # continue

            elif msg == "RESUME":
                running = True
                # continue

            elif msg == "GET_RESULTS":
                #logging.debug(f"[Worker {worker_id}] Received GET_RESULTS")
                result_queue.put((worker_id, {
                    "stream": stream,
                    "staSend": staSend,
                    "staSet": list(staSet),
                    # "delayStaSend": delayStaSend,
                    "streamPick": streamPick,
                    # "delayStream": delayStream
                }))
                #logging.debug(f"[Worker {worker_id}] Sent GET_RESULTS response")
                continue


            # elif isinstance(msg, tuple) and msg[0] == "SET_STREAM":
            #     stream = msg[1]
            #     # continue

            # elif isinstance(msg, tuple) and msg[0] == "SET_STREAM_PICK":
            #     streamPick = msg[1]
            #     # continue

            # elif isinstance(msg, tuple) and msg[0] == "SET_STREAM_DELAY":
            #     delayStream = msg[1]
            #     # continue

            elif msg == "RESET":
                staSend.clear()
                staSet.clear()
                # delayStaSend.clear()
                # continue

            # if not running:
            #     input_queue.put(msg)
            #     time.sleep(0.05)
            #     continue

            elif isinstance(msg, tuple) and msg[0] == "TRIM_STREAMS":
                instructions = msg[1]
                slice_times = instructions.get("slice_times", [])
                delete_ids = set(instructions.get("delete_ids", []))

                for i, tr in enumerate(stream):
                    for s in slice_times:
                        if tr.id == s["id"]:
                            try:
                                stream[i] = tr.slice(starttime=UTCDateTime(s["start"]), endtime=tr.stats.endtime)
                            except Exception as e:
                                stdlogging.error(f"[Worker {worker_id}] Error slicing {tr.id}: {e}")

                stdlogging.debug(f"[Worker {worker_id}] streamPick size before delete: {len(streamPick)}")
                streamPick.traces = [tr for tr in streamPick if tr.id not in delete_ids]
                # streamPick = Stream([tr for tr in streamPick if tr.id not in delete_ids])
                stdlogging.debug(f"[Worker {worker_id}] streamPick size after delete: {len(streamPick)}")

            elif running:
                try:
                    rec_dict = msg
                    
                    # print('STREAM-STREAM-STREAM-STREAM-STREAM')
                    # print(stream)
                    # for tr in stream:
                    #     print(tr)

                    startTime = UTCDateTime(rec_dict["starttime"])
                    endTime =  UTCDateTime(rec_dict["endtime"])
                    sta = rec_dict["station"]
                    net = rec_dict["network"]
                    loc = rec_dict["location"]
                    chan = rec_dict["channel"]
                    sampFreq = rec_dict["sampling_rate"]
                    start_time = pd.to_datetime(str(startTime))
                    end_time = pd.to_datetime(str(endTime))
                    traceID = f'{net}.{sta}.{loc}.{chan}'

                    net_sta = f"{net}.{sta}"

                    if net_sta not in station_list:
                        continue

                    try:
                        data=np.array(rec_dict["data"])
                    except:
                        stdlogging.debug('No data')
                        continue

                    header = {
                        'network': net, 
                        'station': sta, 
                        'location': loc, 
                        'channel': chan, 
                        'npts': len(data), 
                        'sampling_rate': sampFreq,
                        'starttime': UTCDateTime(start_time)
                    }
                    tr = Trace(data=data, header=header)

                    if tr is None or len(tr.data) == 0:
                        continue


                    now = UTCDateTime(Time.GMT())

                    max_lag_seconds = 120
                    data_lag = now - endTime

                    if data_lag > max_lag_seconds:
                        stdlogging.warning(f"[Worker {worker_id}] Delayed data detected for {net}.{sta}.{chan}: lag={data_lag:.2f}s")

                    if now < self.startTime:
                        stdlogging.debug('Waiting for the processing start time %s' % self.startTime)

                    if stream:
                        index, trace = next(((i, t) for i, t in enumerate(stream) if t.stats.network == net and t.stats.station == sta and t.stats.channel == chan),(None, None))
                        if index is not None and trace is not None:
                            tempSt = Stream([trace, tr])
                            tempSt.merge(method=1, fill_value=0)
                            tnew =  tempSt[0]
                            stream[index] = tnew
                        else:
                            stream.append(tr)
                            continue
                        if not trace or (stream[index].stats.endtime - stream[index].stats.starttime) <= (self.windowLength + self.filterShift) or stream[index].stats.endtime <= self.startTime:
                            continue
                    else:
                        stream.append(tr)
                        continue

                    # timeMark1 = self.startTime - self.filterShift
                    # timeMark2 = self.startTime + self.timeShift - self.filterShift
                    # timeMarks = [timeMark1,timeMark2]

                    # if stream[index].stats.endtime < self.startTime +  timedelta(seconds=self.windowLength):
                    #     stream[index].trim(self.startTime-self.filterShift, UTCDateTime(end_time), pad=True, fill_value=0)
                    #     continue
                    # else:
                    #     in_window1 = abs((stream[index].stats.starttime - timeMarks[0])) < 0.5
                    #     in_window2 = abs((stream[index].stats.starttime - timeMarks[1])) < 0.5
                    #     # in_window1 = abs(stream[index].stats.starttime.second - timeMarks[0].second) < 0.5
                    #     # in_window2 = abs(stream[index].stats.starttime.second - timeMarks[1].second) < 0.5
                    #     if not in_window1 and not in_window2:
                    #         stdlogging.info(f"Start time not in timeMarks: {stream[index].stats.starttime} not in {timeMarks}")
                    #         stdlogging.info(f"Trace {stream[index]}")
                    #         newTime = stream[index].stats.starttime.replace(second=timeMarks[0].second) if timeMarks[0].second < stream[index].stats.starttime.second else (stream[index].stats.starttime - 60).replace(second=timeMarks[1].second)
                    #         try:
                    #             stream[index].trim(newTime, UTCDateTime(end_time), pad=True, fill_value=0)
                    #             stdlogging.info(f"Trimmed using closest allowed timestamp {newTime}")
                    #         except Exception as e:
                    #             stdlogging.error("Error trimming trace {} with error {}".format(stream[index],e))


                    if stream[index].stats.endtime < self.startTime +  timedelta(seconds=self.windowLength):
                        stream[index].trim(self.startTime-self.filterShift, UTCDateTime(end_time), pad=True, fill_value=0)
                        continue

                    base_time = self.startTime - self.filterShift
                    base_second = base_time.second

                    expected_seconds = [(base_second + i * self.timeShift) % 60 for i in range(60 // self.timeShift)]

                    trace_second = stream[index].stats.starttime.second + stream[index].stats.starttime.microsecond / 1e6
                    in_window = any(abs(trace_second - s) < 1 for s in expected_seconds)

                    if not in_window:
                        stdlogging.info(f"Start time not in expected timeMarks: {trace_second:.3f}s not near {expected_seconds}")
                        stdlogging.info(f"Trace {stream[index]}")

                        valid_secs = [s for s in expected_seconds if s <= trace_second]
                        chosen_sec = max(valid_secs) if valid_secs else max(expected_seconds)
                        new_start = stream[index].stats.starttime.replace(second=chosen_sec, microsecond=0)

                        try:
                            stream[index].trim(new_start, UTCDateTime(end_time), pad=True, fill_value=0)
                            stdlogging.info(f"Trimmed using aligned second: {new_start}")
                        except Exception as e:
                            stdlogging.error(f"Error trimming trace {stream[index]} with new_start={new_start}: {e}")


                    if stream[index].stats.endtime - stream[index].stats.starttime >= self.windowLength + self.filterShift:
                        index_found = next((j for j, tr in enumerate(streamPick) if tr.id == traceID), None)
                        if index_found is None:
                            # trDelay = UTCDateTime(Time.GMT()) - stream[index].stats.starttime
                            # if trDelay > self.traceDelay:
                            #     index2_found = next((k for k, tr2 in enumerate(delayStream) if tr2.id == traceID), None)
                            #     if index2_found is None:
                            #         delayStream.append(stream[index])
                            #         delayStasPick.add(stream[index].stats.station)
                            #     else:
                            #         delayStream[index2_found] = stream[index]
                            # else:
                            streamPick.append(stream[index])
                            stasPick.add(stream[index].stats.station)   
                        else:
                            streamPick[index_found] = stream[index]
                            stasPick.add(stream[index].stats.station)
                    else:
                        continue
                
                    # streamPick, delayStream = self.checkCommonChan(stream[index],streamPick, delayStream)

                    #if streamPick:
                    #    logging.info("STREAM PICK")
                    #    logging.info("\n".join(str(trace) for trace in streamPick))

                    #if delayStream:
                    #    logging.info("DELAY STREAM PICK")
                    #    logging.info("\n".join(str(trace) for trace in delayStream))

                    # Adding delayed stations with data for all channels avail. to delayStaSend
                    # delayStaSend,delayStasPick = self.checkChann(stream[index],delayStasPick,delayStream,delayStaSend)

                    # Adding stations with data for all channels avail. to staSend
                    staSend,stasPick = self.checkChann(stream[index],stasPick,streamPick,staSend)

                    removed_streams = [trace for trace in stream if (trace.stats.endtime - trace.stats.starttime) > self.timeRemoveTrace]
                    stream = [trace for trace in stream if (trace.stats.endtime - trace.stats.starttime) <= self.timeRemoveTrace]

                    if removed_streams:
                        stdlogging.info(f"Removed streams, (length > {self.timeRemoveTrace} secs):")
                        stdlogging.info("\n".join(str(trace) for trace in removed_streams))

                    # Check delay for stations in memory
                    for i,trace in enumerate(streamPick):
                        sta = streamPick[i].stats.station
                        if sta in staSend and sta not in staSet:
                            delayEnd = UTCDateTime(Time.GMT()) - trace.stats.endtime # Leave here but this time is different than the one use in oneMinuteStream (to evaluate).
                            delayStart = UTCDateTime(Time.GMT()) - trace.stats.starttime # Same than oneMinuteStream this is the correct time to count the delay.
                            if delayEnd >= self.firstWait or delayStart >= self.firstWait + self.windowLength:
                                staSet.add(sta)

                    global_staSend_counts[worker_id] = len(staSend)
                    # global_delayStaSend_counts[worker_id] = len(delayStaSend)
                    global_staSet_counts[worker_id] = len(staSet)

                    # with global_staSend_count.get_lock():
                    #     global_staSend_count.value += len(staSend)

                    # with global_delayStaSend_count.get_lock():
                    #     global_delayStaSend_count.value += len(delayStaSend)

                    # with global_staSet_count.get_lock():
                    #     global_staSet_count.value += len(staSet)
                except Exception as e:
                    stdlogging.exception(f"Unhandled error in worker {worker_id}")
                    # result_queue.put(("ERROR", worker_id, str(e)))
            else:
                time.sleep(0.05)

    def profilesConfig(self):
        profiles = []
		# try:
        profiles = self.configGetStrings('pipelines.profiles')
		# except:
			# logging.error('Error loading profiles' )
			# sys.exit(-1)
        if len(profiles) != 0:
            for profile in profiles:
                profDic = {}
                profDic['name'] = profile
                try:
                    profDic['pickTargetGroup'] = self.configGetString('pipelines.profiles.'+profile+'.pickTargetGroup')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.pickTargetGroup')
                    logging.warning('setting default pickTargetGroup')
                    profDic['pickTargetGroup'] = ''
                    pass

                try:
                    profDic['locTargetGroup'] = self.configGetString('pipelines.profiles.'+profile+'.locTargetGroup')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.locTargetGroup')
                    logging.warning('setting default locTargetGroup')
                    profDic['locTargetGroup'] = ''
                    pass

                try:
                    profDic['authorTarget'] = self.configGetString('pipelines.profiles.'+profile+'.authorTarget')
                except:
                    logging.warning('Error when reading pipelines.profiles.'+profile+'.authorTarget')
                    logging.warning('setting default authorTarget')
                    profDic['authorTarget'] = ''
                    pass

                try:
                    profDic['stations'] = self.configGetStrings('pipelines.profiles.'+profile+'.stations')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.stations')
                    logging.warning('setting stations = ""')
                    profDic['stations'] = ''
                    pass

                try:
                    profDic['region'] = self.configGetString('pipelines.profiles.'+profile+'.region')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.region')
                    logging.warning('setting stations = ""')
                    profDic['region'] = ''
                    pass

                try:
                    profDic['filtersFile'] = self.configGetString('pipelines.profiles.'+profile+'.filtersFile')
                except:
                    logging.warning('error when reading pipelines.profiles.'+profile+'.filtersFile')
                    logging.warning('setting filtersFile = ""')
                    profDic['filtersFile'] = ''
                    pass
                self.profiles.append(profDic)
        else:
            logging.warning('No profiles configured. Nothing to do')
            pass

    def updateStreams(self):
        # streamIDs = self.configStreams()
        streamIDs = self.currentStreams()
        for streamID in streamIDs:
            for sta in self.stations:
                staCheck = False
                if '*' in sta:
                    if (streamID[0] == sta.split('.')[0] and fnmatch.fnmatch(streamID[1], sta.split('.')[1])):
                        staCheck = True
                else:
                    if (streamID[0] == sta.split('.')[0] and streamID[1] == sta.split('.')[1]):
                        staCheck = True
                if staCheck:
                        if streamID[1] in self.stasDict:
                            self.stasDict[streamID[1]].append(streamID[3])
                        else:
                            self.stasDict[streamID[1]] = [streamID[3]]
                        self.initStreams.append(streamID)
                        self.recordStream().addStream(*streamID)
        return

    def configStreams(self):
        streamIDs=[]
        cfg = self.configModule()
        inv = Inventory.Instance().inventory()
        for i in range(cfg.configStationCount()):
            cfg_sta = cfg.configStation(i)
            net = cfg_sta.networkCode()
            sta = cfg_sta.stationCode()
            nets = inv.networkCount()
            for n in range(nets):
                network = inv.network(n)
                if network.code() == net:
                    stas = network.stationCount()
                    for s in range(stas):
                        station = network.station(s)
                        if station.code() == sta:
                            locs = station.sensorLocationCount()
                            for l in range(locs):
                                location = station.sensorLocation(l)
                                loc = location.code()
                                chans = location.streamCount()
                                for c in range(chans):
                                    channel = location.stream(c)
                                    chan = channel.code()
                                    if chan[0:2] in self.whiteChanns:
                                        streamIDs.append([net, sta, loc, chan])
        return streamIDs

    def currentStreams(self):
        now = Time.GMT()
        streamIDs=[]
        inv = Inventory.Instance()
        nets = inv.inventory().networkCount()
        for n in range(nets):
            network = inv.inventory().network(n)
            net = network.code()
            stas = network.stationCount()
            for s in range(stas):
                station = network.station(s)
                sta = station.code()
                station_now = False
                station_now = inv.getStation(network.code(),sta,now)
                if station_now:
                    staCode = station_now.code()
                    locs = station_now.sensorLocationCount()
                    for l in range(locs):
                        location = station_now.sensorLocation(l)
                        loc = location.code()
                        loc_now = False
                        loc_now = inv.getSensorLocation(network.code(),staCode,loc,now)
                        if loc_now:
                            locCode = loc_now.code()
                            chans = location.streamCount()
                            for c in range(chans):
                                channel = location.stream(c)
                                chan = channel.code()
                                if chan[0:2] in self.whiteChanns:
                                    chan_now = False
                                    chan_now = inv.getStream(network.code(),staCode,locCode,chan,now)
                                    if chan_now:
                                        chanCode = chan_now.code()
                                        sid = [net, staCode, locCode, chanCode]
                                        if sid not in streamIDs:
                                            streamIDs.append([net, staCode, locCode, chanCode])
        return streamIDs
                
    def handleRecord(self, rec):

        #logging.debug(f"[Main] handleRecord called at {UTCDateTime.now()}")
        self.recordDispatch(rec)
    
        for i, q in enumerate(self.worker_queues):
            try:
                qsize = q.qsize()
                if qsize > 100:
                    logging.warning(f"[Main] Queue size for worker {i} is high: {qsize}")
                # else:
                #     logging.debug(f"[Main] Queue size for worker {i}: {qsize}")
            except NotImplementedError:
                logging.debug(f"[Main] Queue size not supported for worker {i}")

        threading.Thread(target=self.run_minute_stream_in_thread, daemon=True).start()
        #self.run_minute_stream(rec)
        #logging.debug(f"[Main] Finished run_minute_stream at {UTCDateTime.now()}")

        # for q in self.worker_result_queues:
        #     while not q.empty():
        #         try:
        #             msg = q.get_nowait()
        #             if isinstance(msg, tuple) and msg[0] == "ERROR":
        #                 logging.error(f"[Main] Worker {msg[1]} reported error: {msg[2]}")
        #             elif msg is None:
        #                 logging.warning("[Main] Received unexpected None from result queue.")
        #         except queue.Empty:
        #             break

    def run_minute_stream_in_thread(self):
        if self.stream_lock.locked():
            return

#        if (self.global_staSend_count.value < self.minStasBulk or
#            self.global_staSet_count.value < self.minStasBulk) and \
#            self.global_delayStaSend_count.value < self.minDelayStasBulk:

        total_staSend = sum(self.global_staSend_counts[:])
        total_staSet = sum(self.global_staSet_counts[:])
        # total_delayStaSend = sum(self.global_delayStaSend_counts[:])

        # if self.global_staSend_count.value < self.minStasBulk or self.global_staSet_count.value < self.minStasBulk:
        if total_staSend < self.minStasBulk or total_staSet < self.minStasBulk:
            return

        with self.stream_lock:
            try:
                self.run_minute_stream()
                logging.debug("Running run_minute_stream in new thread")
            except Exception as e:
                logging.error(f"[run_minute_stream_in_thread] Exception: {e}")

    def run_minute_stream(self):

        # streams, staSend,  staSet, delayStaSend, streamPicks, delayStreams = self.gather_worker_results()
        streams, staSend,  staSet, streamPicks = self.gather_worker_results()

        total_stream_traces = sum(len(st) for st in streams)
        total_stream_duration = sum(
            sum((tr.stats.endtime - tr.stats.starttime) for tr in st)
            for st in streams
        )

        total_pick_traces = sum(len(st) for st in streamPicks)
        total_pick_duration = sum(
            sum((tr.stats.endtime - tr.stats.starttime) for tr in st)
            for st in streamPicks
        )

        logging.info(f"[Summary] Total stream traces: {total_stream_traces}, total duration: {total_stream_duration:.1f} s")
        logging.info(f"[Summary] Total streamPick traces: {total_pick_traces}, total duration: {total_pick_duration:.1f} s")
        logging.info(f"[Summary] staSend size: {len(staSend)}, staSet size: {len(staSet)}")

        # logging.info(f"streamPicks, streamPicks, streamPicks, streamPicks, streamPicks")
        # for streamPick in streamPicks:
        #     for i, trace in enumerate(streamPick):
        #         logging.info(f'{trace}')

        # logging.info(f"STREAMS,STREAMS,STREAMS,STREAMS,STREAMS")
        # for stream in streams:
        #     for i, trace in enumerate(stream):
        #         logging.info(f'{trace}')

        self.sendNow = False
        if self.wait:
            delay = UTCDateTime(Time.GMT()) - self.sendTime
            if delay >= self.maxWait:
                self.wait = False
            elif delay >= self.secondWait and len(staSend) > 0:
                self.sendNow = True

        if (len(staSend) >= self.minStasBulk and len(staSet) >= self.minStasBulk and self.wait == False) or self.sendNow:       

            if self.sendNow:
                logging.info(f"Sending now due to second delay")
                self.wait = False

            else:
                logging.info(f"Sending now due to first delay")
                self.sendTime = UTCDateTime(Time.GMT())
                self.wait = True

            tp = 'realtime'
            
            for q in self.worker_queues:
                q.put("PAUSE")
        
            for result_q in self.worker_result_queues:
                while True:
                    status = result_q.get()
                    if status == "PAUSED":
                        break

            self.minuteStream(tp,self.params,streamPicks,streams,staSend)
            #self.reset_worker_output()


        #if len(delayStaSend) >= self.minDelayStasBulk:
        #    for q in self.worker_queues:
        #        q.put("PAUSE")
        #    logging.info(f"Sending now delayed stations")
        #    tp = 'delay'
        #    self.minuteStream(tp,self.params,delayStreams,streams,delayStaSend)
        #    self.reset_worker_output()

    def gather_worker_results(self):

        worker_results = {}

        for q in self.worker_queues:
            q.put("GET_RESULTS")

        logging.warning(f"Started {len(self.worker_queues)} worker processes.")

        for i, result_q in enumerate(self.worker_result_queues):
            try:
                start_time = time.time()
                #worker_id, result = result_q.get(timeout=2.5)
                worker_id, result = result_q.get()
                worker_results[worker_id] = result
                logging.debug(f"[Main] Got result from worker {worker_id} in {time.time() - start_time:.3f} sec")
            except queue.Empty:
                logging.warning(f"[Main] Worker {i} did not respond in time.")
            except Exception as e:
                logging.error(f"[Main] Unexpected error from worker {i}: {e}")

        streams = []
        streamPicks = []
        # delayStreams = []
        combined_staSend = []
        # combined_delayStaSend = []
        combined_staSet = set()

        for i in range(self.numWorkers):
            if i in worker_results:
                result = worker_results[i]
                streams.append(result["stream"])
                streamPicks.append(result["streamPick"])
                # delayStreams.append(result["delayStream"])
                combined_staSend += result.get("staSend", [])
                # combined_delayStaSend += result.get("delayStaSend", [])
                combined_staSet.update(result.get("staSet", []))
            else:
                logging.warning(f"[Main] Missing result from worker {i}, inserting empty defaults")
                streams.append(Stream())
                streamPicks.append(Stream())
                # delayStreams.append(Stream())

        # return streams, combined_staSend, combined_staSet, combined_delayStaSend, streamPicks, delayStreams
        return streams, combined_staSend, combined_staSet, streamPicks

    def reset_worker_output(self):
        for q in self.worker_queues:
            q.put("RESET")
            q.put("RESUME")

    def recordDispatch(self, rec):
        try:
            if not rec.data():
                return

            dataObj = FloatArray.Cast(rec.data())
            if not dataObj:
                logging.debug("No data available in record.")
                return

            # Extract raw data as a list of floats
            data = [dataObj.get(i) for i in range(dataObj.size())]

            # Extract record metadata
            net = rec.networkCode()
            sta = rec.stationCode()
            loc = rec.locationCode()
            chan = rec.channelCode()
            sampFreq = rec.samplingFrequency()
            startTime = rec.startTime()
            endTime = rec.endTime()

            # Create a serializable dictionary
            rec_dict = {
                "network": net,
                "station": sta,
                "location": loc,
                "channel": chan,
                "sampling_rate": sampFreq,
                "starttime": UTCDateTime(startTime).timestamp,
                "endtime": UTCDateTime(endTime).timestamp,
                "data": data
            }

            # Find the corresponding worker group and send the record
            net_sta = f"{net}.{sta}"
            for i, group in enumerate(self.worker_station_groups):
                if net_sta in group:
                    max_lag_seconds = 600
                    now = UTCDateTime.now()
                    rec_end = UTCDateTime(endTime)
                    lag = now - rec_end

                    if lag > max_lag_seconds:
                        logging.warning(f"[recordDispatch] Skipping delayed record {net}.{sta}.{chan}, lag={lag:.1f}s")
                        return
                    
                    self.worker_queues[i].put(rec_dict)
                    break

        except Exception as e:
            logging.error(f"Failed to dispatch record: {e}")

    def checkCommonChan(self, tnew, streamPick, delayStream):
        if not tnew:
            return
        station_tnew = tnew.stats.station
        if any(trace.stats.station == station_tnew for trace in streamPick):
            traces_to_move = [trace for trace in delayStream if trace.stats.station == station_tnew]
            if traces_to_move:
                streamPick += Stream(traces_to_move)
                delayStream = Stream([trace for trace in delayStream if trace.stats.station != station_tnew])
        return(streamPick,delayStream)

    def minuteStream(self,tp,params,streamPicks,streams,staSend):
        oneMinuteStream = Stream()

        # Process the one-minute stream
        for streamPick in streamPicks:
            toRemove = []
            for i,trace in enumerate(streamPick):
                if streamPick[i].stats.station in staSend:
                    duration = streamPick[i].stats.endtime - streamPick[i].stats.starttime
                    Nminutes = int(duration // self.windowLength)
                    if Nminutes > 0:
                        startMinute = streamPick[i].stats.starttime
                        endMinute = streamPick[i].stats.starttime + (Nminutes*self.windowLength) + self.filterShift
                        if endMinute > streamPick[i].stats.endtime and Nminutes > 1:
                            Nminutes -= 1
                            endMinute = startMinute + (Nminutes * self.windowLength) + self.filterShift
                        if Nminutes > 0:
                            traceMin = trace.slice(
                                            starttime=startMinute,
                                            endtime=endMinute
                                            )
                            oneMinuteStream.append(traceMin)
                            toRemove.append(i)
                        else:
                            logging.info(f"This should never happen, Trying to send a trace with less than a minute duration {streamPick[i]}")
                    else:
                        logging.info(f"This should never happen, Trying to send a trace with less than a minute duration{streamPick[i]}")
                        continue

            # for i in sorted(toRemove, reverse=True): del streamPick[i]

        for i in range(self.numWorkers):
            trim_instructions = {
                "slice_times": [], 
                "delete_ids": []
            }

            for tr in oneMinuteStream:
                trace_id = tr.id

                trim_instructions["slice_times"].append({
                    "id": trace_id,
                    "start": tr.stats.endtime - self.timeShift - self.filterShift
                })

                if tr.stats.station in staSend:
                    trim_instructions["delete_ids"].append(trace_id)

            self.worker_queues[i].put(("TRIM_STREAMS", trim_instructions))


        # for stream in streams:
        #     for i,trace in enumerate(stream):
        #         for tr in oneMinuteStream:
        #             if trace.stats.network == tr.stats.network and trace.stats.station == tr.stats.station and trace.stats.channel == tr.stats.channel:
        #                 try:
        #                     # logging.info(f"Stream before slice {stream[i]}")
        #                     stream[i] = trace.slice(
        #                         starttime=tr.stats.endtime - self.timeShift - self.filterShift,
        #                         endtime=stream[i].stats.endtime,
        #                         )
        #                     # logging.info(f"Stream after slice {stream[i]}")
        #                     # logging.info(f"Endtime oneminute-tr{tr.stats.endtime}")
        #                     # logging.info(f"Endtime stream(i){stream[i].stats.endtime}")
        #                 except:
        #                     logging.error(f"Error when slice the trace %s" % stream[i])

        # if tp == 'realtime':
        #     for i, updated_streamPick in enumerate(streamPicks):
        #         self.worker_queues[i].put(("SET_STREAM_PICK", updated_streamPick))
        # elif tp == 'delay':
        #     for i, updated_streamPick in enumerate(streamPicks):
        #         self.worker_queues[i].put(("SET_STREAM_DELAY", updated_streamPick))

        # for i, updated_stream in enumerate(streams):
        #     self.worker_queues[i].put(("SET_STREAM", updated_stream))

        self.reset_worker_output()

        logging.info(f"ONE MINUTE STREAM")
        for i, trace in enumerate(oneMinuteStream):
            logging.info(f'{trace}')

        for i in range(self.numWorkers):
            self.global_staSend_counts[i] = 0
            # self.global_delayStaSend_counts[i] = 0
            self.global_staSet_counts[i] = 0

        # with self.global_staSend_count.get_lock():
        #     self.global_staSend_count.value = 0

        # with self.global_delayStaSend_count.get_lock():
        #     self.global_delayStaSend_count.value = 0

        # with self.global_staSet_count.get_lock():
        #     self.global_staSet_count.value = 0

        # Run EQCCT picker
        # self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStream,params))
        #self.tasks_ids.append(sceqcct.run_picker.remote(startMinute,endMinute,self.oneMinuteStream,params))
        self.task_queue.put(sceqcct.run_picker.remote(startMinute, endMinute,oneMinuteStream,params))
        self.log_queue_size()
        logging.info(f"Task for data {startMinute}, {endMinute}, {oneMinuteStream} assigned.")
        logging.info(f"The main process continues to generate and launch tasks.")

        # resultados = ray.get(self.tasks_ids)
        # print("Resultados de las tareas secundarias:", resultados)

        # self.sendTask(startMinute,endMinute,self.oneMinuteStream)
        # self.run_picker()

        # oneMinuteStream = Stream()
        # staSend = []

    def checkChann(self, tnew, stasPick, streamPick, staSend):
        if not tnew:
            return  
        station_tnew = tnew.stats.station
        if station_tnew in stasPick:
            checkSta = all(
                any(trace.stats.station == station_tnew and recChann == trace.stats.channel for trace in streamPick)
                for recChann in self.stasDict.get(station_tnew, [])
            )
            if checkSta and station_tnew not in staSend:
                staSend.append(station_tnew)
                stasPick.discard(station_tnew)
        return(staSend,stasPick)

    @staticmethod
    @ray.remote
    def run_picker(start_time,end_time,stream,params):
        
        # Create working dirs 
        #Path(os.path.join(self.ei.shareDir(), "sceqcct/results/picks")).mkdir(exist_ok=True, parents=True)
        #Path(os.path.join(self.ei.shareDir(), "sceqcct/results/logs/picker")).mkdir(exist_ok=True, parents=True)
        #pathResutls = os.path.join(self.ei.shareDir(), "sceqcct/results")

        # Load EQCCT model                                        
        # model = self.predictor.load_model(self.eqcct_Pmodel, self.eqcct_Smodel, f"{pathResutls}/model.log")

        # Get picker tasks
        start_time = UTCDateTime(start_time)
        end_time = UTCDateTime(end_time)
        times_list = [[start_time, end_time]]

        #log_file_run = f"{pathResutls}/logs/picker/eqcct_run.log"
        #logging.info(f"[{datetime.now()}] Running EQCCT picker. Log file: {log_file_run}")

        # Prepare tasks for EQCCT picking
        stations = []
        for trace in stream:
            # station = f'{trace.stats.network}.{trace.stats.station}.{trace.stats.location}.{trace.stats.channel}'
            station = f'{trace.stats.network}.{trace.stats.station}'
            if station not in stations:
                stations.append(station)


        tasks = [[f"({i+1}/{len(times_list)})", times_list[i][0].strftime(format="%Y%m%dT%H%M%SZ"), times_list[i][1].strftime(format="%Y%m%dT%H%M%SZ"), stations] for i in range(len(times_list))]

        # Submit tasks to ray in a queue
        # tasks_queue = []
        # logging.debug(f"[{datetime.now()}] Started EQCCT picking process.\n")
        # for i in range(len(tasks)):
        #     while True:
        #         # Add new task to queue while max is not reached
        #         if len(tasks_queue) < self.maxTimeTasksQueue:
        #             tasks_queue.append(self.picker.remote(tasks[i],stream))
        #             break
        #         # If there are more tasks than maximum, just process them
        #         else:
        #             tasks_finished, tasks_queue = ray.wait(tasks_queue, num_returns=1, timeout=None)
        #             for finished_task in tasks_finished:
        #                 log_entry = ray.get(finished_task)
        #                 logging.info(log_entry + "\n")

        # # After adding all the tasks to queue, process what's left
        # while tasks_queue:
        #     tasks_finished, tasks_queue = ray.wait(tasks_queue, num_returns=1, timeout=None)
        #     for finished_task in tasks_finished:
        #         log_entry = ray.get(finished_task)
        #         logging.info(log_entry + "\n")
        # ray.shutdown()

        logging.debug('Sending stations to EQCCT %s'% stations)
        tasks = [[f"({i+1}/{len(times_list)})", times_list[i][0], times_list[i][1], stations] for i in range(len(times_list))]

        # Run picker
        for task in tasks:
            picks = sceqcct.picker(task,stream,params)

        return picks

    def log_queue_size(self):
        current_size = self.task_queue.qsize()
        if current_size != self.last_queue_size:
            logging.info(f"Pending tasks on the monitor: {current_size}")
            self.last_queue_size = current_size

    def monitorTasks(self):
        results = []
        
        #while tasks_ids or not self.tasks_ids_end.is_set():
        #while not self.tasks_ids_end.is_set() or (not self.task_queue.empty() and not self.tasks_ids_end.is_set()):
        while not self.tasks_ids_end.is_set() or not self.task_queue.empty():
            # We check the completed results without blocking
            try:
                tasks_ids = []

                for _ in range(10):
                    try:
                        tasks_ids.append(self.task_queue.get(timeout=1))
                        # tasks_ids.append(self.task_queue.get_nowait())
                    except queue.Empty:
                        break

                # while not self.task_queue.empty():
                    # tasks_ids.append(self.task_queue.get_nowait())

                if tasks_ids:
                    results_ready, remaining_tasks = ray.wait(tasks_ids, num_returns=min(10, len(tasks_ids)), timeout=1)
                    #results_ready, tasks_ids[:] = ray.wait(tasks_ids, num_returns=len(tasks_ids), timeout=1)
                
                    # We process the results that are ready
                    for result_id in results_ready:
                        try:
                            result = ray.get(result_id)
                            # logging.info(f"Result received on the monitor: {result}")
                            self.scPhase(result)
                            results.append(result)
                        except Exception as task_error:
                            logging.error("Error while processing task {}".format(task_error))
                            logging.error(traceback.format_exc())
                    
                    for task in remaining_tasks:
                        self.task_queue.put(task)

                self.log_queue_size()

            except Exception as e:
                logging.error("Task Monitor Error {}".format(e))
                logging.error(traceback.format_exc())

        logging.info(f"The monitor has processed all the results.")
        return results

    @staticmethod
    def picker(task,stream,params):

        picks = None
        #eqcctPath = os.path.join(params['share_path'], "sceqcct/tools")
        ## share_path = os.path.abspath("../share/sceqcct/tools")
        #sys.path.append(eqcctPath)
        #predictor = __import__('predictor')

        pos, starttime, endtime, stations = task
        # begin = starttime.strftime(format="%Y%m%dT%H%M%SZ")
        # end = endtime.strftime(format="%Y%m%dT%H%M%SZ")

        #log_file = os.path.join(self.ei.shareDir(), f"sceqcct/results/logs/picker/{begin}_{end}.log")
        #logging.info(f"[{datetime.now()}] Running EQCCT picker {pos}. Log file: {log_file}")

        #output_eqcct = os.path.join(self.ei.shareDir(), f"sceqcct/results/eqcct/{begin}_{end}")
        
        picks = mseed_predictor(stream = stream,
                        filterShift    = params['filterShift'],
                        # output_dir    = output_eqcct,
                        stations2use  = stations,
                        P_threshold   = params['P_threshold'],
                        S_threshold   = params['S_threshold'],
                        normalization_mode = params['normalization_mode'],
                        overlap       = params['overlap'],
                        batch_size    = params['batch_size'],
                        overwrite     = params['overwrite'],
                        # log_file      = log_file,
                        # model         = model,
                        p_model       = params['p_model'],
                        s_model       = params['s_model'],
                        numCPUs       = params['numCPUs'],
                        gpu_id        = params['gpu_id'],
                        gpu_limit     = params['gpu_limit'],
                        maxStsTasksQueue = params['maxStsTasksQueue'],
                        stations_filters = params["df_filters"])
        
        return picks
        # picks2xml(input_file=output_eqcct, output_file=f"./{pathResutls}/picks/picks_{begin}_{end}.xml", ai='eqcc', thr_dict=self.probThreshold)

    def scPhase(self,picks):
        for pickData in picks:
            logging.info(str(pickData))
            if pickData['PdateTime']:
                PpickID = self.scPick(pickData, 'P', 'PdateTime', 'p_prob')
                if pickData['SdateTime']:
                    pickData['PpickID'] = PpickID
                    self.scPick(pickData, 'S', 'SdateTime', 's_prob')

    def scPick(self,pickData,phaseHint,strDate,strProp):
        pick = Pick.Create()
        Ptime = pickData[strDate]
        scPtime = Ptime.strftime('%Y-%m-%d %H:%M:%S.%f')
        scPtime = Time()
        scPtime.set(Ptime.year,Ptime.month,Ptime.day,Ptime.hour,Ptime.minute,Ptime.second,Ptime.microsecond)
        timeQ = TimeQuantity()
        timeQ.setValue(scPtime)
        # timeQ.setUncertainty(float(pickData[strProp]))
        pick.setTime(timeQ)
        propValue = str(pickData[strProp])
        comm = Comment()
        comm.setId('eqcctProb')
        comm.setText(propValue)
        ci = CreationInfo()
        ci.setCreationTime(Time().GMT())
        comm.setCreationInfo(ci)
        pick.add(comm)
        net_sta = f"{pickData['trace_name'].split('.')[0]}.{pickData['trace_name'].split('.')[1]}"
        staName = pickData['trace_name'].split('.')[1]
        freqmin = 1.0
        freqmax = 45.0
        filterID = f"BW(2,{freqmin},{freqmax})"
        for profile in self.profiles:
            if net_sta in profile["stations"]:
                try:
                    filters = pd.read_csv(profile["filtersFile"], header=None, names=["sta", "hp", "lp"])
                    freqmin = filters[filters.sta == staName].iloc[0]["hp"]
                    freqmax = filters[filters.sta == staName].iloc[0]["lp"]
                    filterID = f"BW(2,{freqmin},{freqmax})"
                except:
                    pass
        # print(pick.time().value())
        # print(pick.time().uncertainty())
        wfID = WaveformStreamID()
        wfID.setNetworkCode(pickData['trace_name'].split('.')[0])
        wfID.setStationCode(pickData['trace_name'].split('.')[1])
        wfID.setLocationCode(pickData['trace_name'].split('.')[2])
        wfID.setChannelCode(pickData['trace_name'].split('.')[3])
        pick.setWaveformID(wfID)
        phase = Phase()
        phase.setCode(phaseHint)
        pick.setPhaseHint(phase)
        pick.setMethodID('EQCCT')
        pick.setEvaluationMode(1)
        pick.setCreationInfo(ci)
        
        filterAndProp = "  ".join([f"prop:  {propValue}", f"filt: {filterID}"])
        pick.setFilterID(filterAndProp)

        logging.debug("Pick produced by EQCCT %s.%s" % (pick.waveformID().networkCode(), pick.waveformID().stationCode()))
        logging.debug('%s' % pick)
            
        self.sendPick(pick)
        self.sendComt(comm, pick)

        # pickRef = PickReference()
        if phaseHint == 'S':
            # pickRef.setPickID(pickData['PpickID'])
            comt = Comment()
            comt.setId('RefPickID')
            comt.setText(str(pickData['PpickID']))
            ci = CreationInfo()
            ci.setCreationTime(Time().GMT())
            comt.setCreationInfo(ci)
            # print(pick.comment(0).id())
            # print(pick.comment(0).text())    
            pick.add(comt)
            self.sendComt(comt, pick)
        else:
            # pickRef.setPickID(pick.publicID())
            pass

        return pick.publicID()

    def sendComt(self, comt, pick):
        op = OP_ADD
        if comt:
            n = Notifier(pick.publicID(), op, comt)
            msg = NotifierMessage()
            msg.attach(n)
            out = self.connection().send(msg)
            logging.debug("Comment sent? %s" % "Yes" if out else "No, failed" )
            # comtObj = Comment.Cast(comt)
            # Notifier.Enable()
            # n = Notifier("EventParameters", op, comtObj)
            # msg = NotifierMessage()
            # msg.attach(n)
            # out = self.connection().send(msg)
            # logging.debug("Comment sent? %s" % "Yes" if out else "No, failed" )

    def sendPick(self, pick):
        op = OP_ADD
        if pick:
            pickObj = Pick.Cast(pick)
            Notifier.Enable()
            # ep = EventParameters()
            # ep.add(pickObj)
            n = Notifier("EventParameters", op, pickObj)
            msg = NotifierMessage()
            msg.attach(n)
            for profile in self.profiles:
                logging.debug("PROFILE %s" % profile["name"])
                logging.debug("wfID %s" % pickObj.waveformID())
                logging.debug("net %s" % pickObj.waveformID().networkCode())
                netSta = f"{pickObj.waveformID().networkCode()}.{pickObj.waveformID().stationCode()}"
                if netSta in profile["stations"]:
                    target = profile["pickTargetGroup"]
                    if target:
                        logging.debug("Trying to send pick %s to %s" % (netSta,target))
                        out = self.connection().send(target,msg)
                        logging.debug("Pick sent? %s" % "Yes" if out else "No, failed" )
                    else:
                        logging.debug("Trying to send pick %s to default target" % (netSta))
                        out = self.connection().send(msg)
                        logging.debug("Pick sent? %s" % "Yes" if out else "No, failed" )

        # if pickRef:
        #     pickRefObj = PickReference.Cast(pickRef)
        #     Notifier.Enable()
        #     n2 = Notifier("EventParameters", op, pickRefObj)
        #     msg2 = NotifierMessage()
        #     msg2.attach(n2)
        #     out2 = self.connection().send(msg2)
        #     logging.debug("PickReference sent? %s" % "Yes" if out2 else "No, failed" )
        # return out

    def changeOrigStatus(self, origin, EvalStat):
        try:
            origin.setEvaluationStatus(EvalStat)
            logging.debug("Changing origin %s evaluation status %s" % (origin.publicID(),EvalStat))
        except Exception as e:
            logging.error("Error changing origin evaluation status {} with error {}".format(origin.publicID(),e))
        return origin

    def qualityCheckOrigin(self, origin):

        latUnc = origin.latitude().uncertainty()
        lonUnc = origin.longitude().uncertainty()
        try:
            depthUnc = origin.depth().uncertainty()
        except ValueError:
            depthUnc = 0
        depth = origin.depth().value()
        azGap = origin.quality().azimuthalGap()

        thresholds = {'latUnc': self.latUncTHR, 'lonUnc': self.lonUncTHR, 'depthUnc': self.depthUncTHR, 'depth': self.depthTHR, 'azGap': self.azGapTHR}
        values = {'latUnc': latUnc, 'lonUnc': lonUnc, 'depthUnc': depthUnc, 'depth': depth, 'azGap': azGap}

        exceeding = {key: value for key, value in values.items() if value > thresholds[key]}
        
        if exceeding:
            logging.debug(f"Values exceeding thresholds: {exceeding}")
            return False
        else:
            logging.debug(f"Origin fulfills thresholds: {exceeding}")
            return True

    def geographicCheckOrigin(self, region, origin):
        if isinstance(region, str):
            if region.split('.')[-1] == 'bna':
                try:
                    within = self.inPolygon(region, origin.latitude().value(), origin.longitude().value())
                    return within
                except FileNotFoundError:
                    logging.error(f'\n\n\t {region} file not found\n\n')
                    return False
            elif len(region.split(',')) == 4:
                quadrant = region.split(',')
                quadrant = tuple(map(float, quadrant))
                return self.inQuadrant(quadrant, origin)
            else:
                logging.error(f'Provide a quadrant or bna file')
                return False
        elif isinstance(region, tuple):
            return self.inQuadrant(quadrant, origin)
        else:
            logging.error(f'Provide a quadrant or bna file')
            return False

    def inPolygon(self, fileName, lat, lon) -> bool:
        self.fs = geo.GeoFeatureSet()
        if not self.fs.readBNAFile(fileName, None):
            logging.error('Impossible to open the bna file %s'%fileName)
            return False
        
        feature = self.fs.features()[0]
        closed = feature.closedPolygon()
        if not closed:
            logging.error('Please fix this: Polygon %s does not not exist or is not closed'%fileName)
        
        coordinates = geo.GeoCoordinate(lat,lon)
        within = feature.contains(coordinates)
        if not within:
            logging.debug('Origin (lat: %s and lon: %s) is not within polygon %s.' % (lat,lon,fileName))

        return(within)

    def inQuadrant(self, quadrant: tuple, origin) -> bool:
        """Check if event location is in a quadrant

        Parameters
        ----------
        quadrant : Tuple
            Quadrant to check in format (lat_min, lat_max, lon_min,  lon_max)

        Returns
        -------
        bool
            True if event is in quadrant, False if not
        """
        assert len(quadrant) == 4, 'Quadrant must be a tuple with 4 elements'
        assert quadrant[0] < quadrant[1], 'The minimum latitude must be less than the maximum latitude'
        assert quadrant[2] < quadrant[3], 'The minimum longitude must be less than the maximum longitude'
        return (quadrant[0] <= origin.latitude().value() <= quadrant[1]
                and quadrant[2] <= origin.longitude().value() <= quadrant[3])

    def handleOrigin(self, org, op=OP_UPDATE):
        """
        Test origins
        """
        logging.debug("Received origin %s" % org.publicID())
        author = org.creationInfo().author()
        profNames = (list(map(lambda p: p['name'], self.profiles)))
        logging.debug("PROFILES %s" % profNames)
        for profile in self.profiles:
            if author == profile["authorTarget"]:
                try:
                    polygon = profile["region"]
                    if polygon != 'none':
                        checkReg = self.geographicCheckOrigin(polygon, org)
                    else:
                        checkReg = True
                    checkQc = self.qualityCheckOrigin(org)
                except:
                    logging.error("Error when performing quality checks for the origin %s" % org.publicID())
                    return
                if checkQc and checkReg:
                    org = self.changeOrigStatus(org,self.TrueOrgEvalStat)
                    locTarget = profile["locTargetGroup"]
                    if org and locTarget:
                        msg = NotifierMessage()
                        n = Notifier("EventParameters", op, org)
                        msg.attach(n)
                        out = self.connection().send(locTarget,msg)
                        logging.debug("Evaluation status updated? %s" % "Yes" if out else "No, failed" )
                # else:
                    # org = self.changeOrigStatus(org,self.FalseOrgEvalStat)
            else:
                logging.debug("Author %s not in sceqcct profile %s" %(author,profile["name"]))
            
    def addObject(self, parentID, object):
        """
        Call-back function if a new object is received.
        """
        origin = Origin.Cast(object)
        if origin:
            self.handleOrigin(origin, op=OP_UPDATE)

    def updateObject(self, parentID, object):
        """
        Call-back function if an object is updated.
        """
        origin = Origin.Cast(object)
        if origin:
            self.handleOrigin(origin, op=OP_UPDATE)

    def handleClose(self):
        self.tasks_ids_end.set()
        self.monitorThread.join()
        logging.info(f'The main process has completed all tasks.')
        ray.shutdown()

    def handleAutoShutdown(self):
        self.tasks_ids_end.set()
        self.monitorThread.join()
        logging.info(f'The main process has completed all tasks.')
        ray.shutdown()

    def run(self):
        return StreamApplication.run(self)

def set_cpu_affinity(cores):
    """Set CPU affinity for the current process."""
    p = psutil.Process(os.getpid())
    p.cpu_affinity(cores)

if __name__ == '__main__':
    import sys
    script_name = os.path.basename(sys.argv[0])
    core_mappings = {
#        "sceqcctMID": list(range(0, 30)),
#        "sceqcctMIDEF": list(range(0, 40)),
        "sceqcctALL": list(range(4, 46)),
    }
    listCores = core_mappings.get(script_name, list(range(0, 4)))
    set_cpu_affinity(listCores)
    app = sceqcct()
    sys.exit(app())
